<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Srijan Das</title>
  
  <meta name="author" content="Srijan Das">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Srijan Das</name>
              </p>
              <p>I am a Postdoctoral Associate at Stony Brook University. I am a member of the Robotics Lab in <a href="http://michaelryoo.com/">Michael Ryoo's</a> team.
              </p>
              <p>
                At SBU, I' am working on Video Representation Learning, and Robotic Vision. Before this, I completed my Ph.D. in Computer Science at INRIA, Sophia Antipolis, France under the supervision of <a href="http://www-sop.inria.fr/members/Francois.Bremond/">Francois Bremond</a> and <a href="http://www-sop.inria.fr/members/Monique.Thonnat/">Monique Thonnat</a>.
                My Ph.D. thesis is on <a href="https://hal.archives-ouvertes.fr/tel-02973812/document">¨Spatio-temporal attention mechanisms for Action Recognition¨</a> and click <a href="https://www.youtube.com/watch?v=HIsqMt9dA78">here</a> to watch my Defense Presentation.
                I did my Post-Grad in Computer Science from the <a href="https://www.nitrkl.ac.in/">National Institute of Technology (NIT), Rourkela</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:srijan.das@stonybrook.edu">Email</a> &nbsp/&nbsp
                <a href="data/srijan_CV.pdf">CV</a> &nbsp/&nbsp
                <a href="data/srijan-bio.txt">Bio</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=ZDTF5AEAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/srijandas07">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/srijandas07/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/srijan_pic.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/srijan_circle.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in computer vision, machine learning, deep learning, and image processing. I have been mostly working on video representation learning including spatio-temporal attention mechanisms, cross-modal attention mechanisms, cross-modal knowledge distillation, and self-supervised learning for applications like action classification in trimmed videos, temporal action detection in untrimmed videos, video retrieval, anaomaly detection, and deepfake detection. Representative papers are <span class="highlight">highlighted</span>.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr onmouseout="viewclr_stop()" onmouseover="viewclr_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/viewclr.png' hieght="150px" width="200px">
              </div>
              <script type="text/javascript">
                function viewclr_start() {
                  document.getElementById('rawnerf_image').style.opacity = "1";
                }

                function rawnerf_stop() {
                  document.getElementById('viewclr_image').style.opacity = "0";
                }
                viewclr_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2112.03905">
                <papertitle>ViewCLR: Learning Self-supervised Video Representation for Unseen Viewpoints</papertitle>
              </a>
              <br>
              <strong>Srijan Das</strong>, and Michael S. Ryoo
              <br>
							<em>Arxiv Pre-print</em>, December 2021
              <br>
              <a href="https://arxiv.org/abs/2112.03905">arXiv</a>
              <p></p>
              <p>
								A framework for learning self-supervised video representation that is invariant to unseen camera viewpoints.</p>
            </td>
          </tr>

        <tr onmouseout="mstct_stop()" onmouseover="mstct_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/mstct.png' height=140px width=185px>
              </div>
              <script type="text/javascript">
                function mstct_start() {
                  document.getElementById('MSTCT').style.opacity = "1";
                }
                function mstct_stop() {
                  document.getElementById('MSTCT').style.opacity = "0";
                }
                mstct_stop()
              </script>
           </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <a href="https://arxiv.org/pdf/2112.03902.pdf">
                     <papertitle>MS-TCT: Multi-Scale Temporal ConvTransformer for Action Detection</papertitle>
               </a>
              <br>
              Rui Dai, <strong>Srijan Das</strong>, Kumara Kahatapitiya, Michael S. Ryoo, Francois Bremond.
              <br>
              <em>Arxiv Pre-print</em>, December 2021
              <br>
              <p> A ConvTransformer network that explores global and local temporal relations at multiple resolutions.</p>
            </td>
          </tr>

          <tr onmouseout="vpn++_stop()" onmouseover="vpn++_start()"  bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">

                <img src='images/vpn++.png' height=150px width=200px>
              </div>
              <script type="text/javascript">
                function mip360_start() {
                  document.getElementById('vpn++_image').style.opacity = "1";
                }

                function mip360_stop() {
                  document.getElementById('vpn++_image').style.opacity = "0";
                }
                vpn++_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2105.08141">
                <papertitle>VPN++: Rethinking Video-Pose embeddings for understanding Activities of Daily Living</papertitle>
              </a>
              <br>
              <strong>Srijan Das</strong>, Rui Dai, Di Yang, Francois Bremond,
              <br>
							<em>TPAMI</em>, 2021 (DOI: 10.1109/TPAMI.2021.3127885)
              <br>
              <a href="https://arxiv.org/abs/2105.08141">arXiv</a>
              /
              <a href="https://github.com/srijandas07/vpnplusplus">code</a>
              <p></p>
              <p>VPN++ is an extension of our VPN model (ECCV 2020). VPN++ hallucinates pose driven features while not requiring costly 3D Poses at inference.</p>
            </td>
          </tr> 

          <tr onmouseout="bmvc21_stop()" onmouseover="bmvc21_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/ctrn.png' height=145px width=185px>
              </div>
              <script type="text/javascript">
                function bmvc21_start() {
                  document.getElementById('bmvc21_image').style.opacity = "1";
                }
                function bmvc21_stop() {
                  document.getElementById('bmvc21_image').style.opacity = "0";
                }
                tsu_stop()
              </script>
           </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <a href="https://www.bmvc2021-virtualconference.com/conference/papers/paper_0133.html">
                     <papertitle>CTRN: Class Temporal Relational Network for Action Detection</papertitle>
               </a>
              <br>
              Rui Dai, <strong>Srijan Das</strong>, Francois Bremond.
              <br>
              <em> <!--The British Machine Vision Conference, 2021. -->
                <strong>BMVC 2021, Oral</strong></em>
              <br>
            </td>
          </tr>


					

          <tr onmouseout="pdan_stop()" onmouseover="pdan_start()">
            <td style="padding:20px;width:25%;vertical-align:middle";"text-align: center">
              <div class="one">
                <img src='images/pdan.png' height=125px width=185px>
              </div>
              <script type="text/javascript">
                function pdan_start() {
                  document.getElementById('pdan_image').style.opacity = "1";
                }
                function pdan_stop() {
                  document.getElementById('pdan_image').style.opacity = "0";
                }
                vpn_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/WACV2021/papers/Dai_PDAN_Pyramid_Dilated_Attention_Network_for_Action_Detection_WACV_2021_paper.pdf">
                <papertitle>PDAN: Pyramid Dilated Attention Network for Action Detection.</papertitle>
              </a>
              <br>
              Rui Dai, <strong>Srijan Das</strong>, Luca Minciullo, Lorenzo Garattoni, Gianpiero Francesca and Francois Bremond.
              <br>
              <em><!--IEEE Winter Conference on Applications of Computer Vision</em>, 2021. -->
                <strong>WACV 2021</strong>
              <br>
              <a href="https://github.com/dairui01/PDAN">Code</a> / <a href="https://www.youtube.com/watch?v=lvEcjufIkdo">Video</a> / <a href="research/RLDL_poster.pdf">Poster</a>
            </td>
          </tr>

	
  <tr onmouseout="vpn_stop()" onmouseover="vpn_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/vpn.png' height=145px width=185px>
              </div>
              <script type="text/javascript">
                function vpn_start() {
                  document.getElementById('vpn_image').style.opacity = "1";
                }
                function vpn_stop() {
                  document.getElementById('vpn_image').style.opacity = "0";
                }
                vpn_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2007.03056">
                <papertitle>VPN: Learning Video-Pose Embedding for Activities of Daily Living</papertitle>
              </a>
              <br>
              <strong>Srijan Das</strong>, Saurav Sharma, Rui Dai, Francois Bremond, Monique Thonnat.
              <br>
              <em><!--16th European Conference on Computer Vision</em>, 2020.-->
                <strong>ECCV 2020</strong>
              <br>
              <br>
              <a href="https://github.com/srijandas07/VPN">Code</a>
            </td>
          </tr>
	

					


					

</body>

</html>
