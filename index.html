<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Srijan Das</title>
  
  <meta name="author" content="Srijan Das">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">

   <style>
  body {
    font-family: 'Inter', sans-serif;
    background-color: #f4f4f9;
    color: #222;
    line-height: 1.6;
    margin: 0;
    padding: 0;
  }

  header, footer {
    background-color: #003366;
    color: white;
    padding: 20px;
    text-align: center;
  }

  header h1 {
    margin: 0;
    font-size: 2.5em;
  }

  header p {
    margin: 0;
    font-size: 1.2em;
  }

  main {
    max-width: 1000px;
    margin: 30px auto;
    padding: 20px;
    background: white;
    box-shadow: 0 0 15px rgba(0, 0, 0, 0.05);
    border-radius: 10px;
  }

  name {
    font-size: 2.5em;
    font-weight: 700;
    color: #1a1a1a;
  }

  heading {
    font-size: 1.8em;
    font-weight: 600;
    color: #2a2a2a;
    border-bottom: 2px solid #ddd;
    display: block;
    padding-bottom: 4px;
    margin-top: 40px;
  }

  subheading {
    font-size: 1.3em;
    font-weight: 500;
    color: #444;
    margin-top: 10px;
    display: block;
  }

  a {
    color: #0056b3;
    text-decoration: none;
  }

  a:hover {
    text-decoration: underline;
  }

  img.profile {
    border-radius: 50%;
    width: 180px;
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
  }

  ul {
    padding-left: 20px;
  }

  li {
    margin-bottom: 8px;
  }

  .publication, .news-item {
    background: #f0f0f0;
    padding: 8px 12px;
    border-radius: 6px;
    margin-bottom: 4px;
  }

  details summary {
    font-weight: bold;
    font-size: 1.1em;
    cursor: pointer;
    margin: 10px 0;
  }

  .section {
    margin-bottom: 20px;
  }
</style>

</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Srijan Das</name>
              </p>
              <p>I am an Assistant Professor in the Department of Computer Science at the University of North Carolina at Charlotte. At UNC Charlotte, I am working on Video Representation Learning, and Robotic Vision. I am a member of the  <a href="https://cws.charlotte.edu/stage-ai4health/">AI4Health Center</a> and one of the founding members of the Charlotte Machine Learning Lab (<a href="https://charlotteml.github.io/">CharMLab</a>) at UNC Charlotte.
              </p>
              <p>
                  Before this, I was a Postdoctoral Associate at Stony Brook University under the supervision of <a href="http://michaelryoo.com/">Michael S. Ryoo</a>. In 2020, I completed my Ph.D. in Computer Science at INRIA, Sophia Antipolis, France under the supervision of <a href="http://www-sop.inria.fr/members/Francois.Bremond/">Francois Bremond</a> and <a href="http://www-sop.inria.fr/members/Monique.Thonnat/">Monique Thonnat</a>.
                My Ph.D. thesis is on <a href="https://hal.archives-ouvertes.fr/tel-02973812/document">¨Spatio-temporal attention mechanisms for Action Recognition¨</a> and click <a href="https://www.youtube.com/watch?v=HIsqMt9dA78">here</a> to watch my Defense Presentation.
                I did my Post-Grad in Computer Science from the <a href="https://www.nitrkl.ac.in/">National Institute of Technology (NIT), Rourkela</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:sdas24@charlotte.edu">Email</a> &nbsp/&nbsp
                <a href="data/srijan_CV_2025.pdf">CV</a> &nbsp/&nbsp
                <a href="data/srijan-bio.txt">Bio</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=ZDTF5AEAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/srijandas07">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/srijandas07/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/srijan_circle_milan.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/srijan_circle_milan.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research focuses on representation learning in long videos, leveraging vision-language models, multimodal inputs, and both egocentric and exocentric viewpoints. I am also interested in vision-language-action models for robot learning and the use of diffusion models for video generation.
            </td>
          </tr>
        </tbody></table>

       <!-- Previous HTML up to before News section remains unchanged -->

<section class="section">
  <heading>News</heading>
  <details open>
    <summary>2025</summary>
    <div class="news-item"><b>Jun</b>: 2 papers accepted to ICCV 2025 (<a href="https://arxiv.org/abs/2504.01009">Gecko</a> and <a href="https://arxiv.org/abs/2412.13393">MaskHand</a>).</div>
    <div class="news-item"><b>May</b>: Outstanding Reviewer for CVPR 2025.</div>
    <div class="news-item"><b>May</b>: Our research <a href="https://www.wsoctv.com/news/local/unc-charlotte-professor-develops-ai-video-tool-monitor-home-safety/FLPQL4EWYNDAPJ3CRRTY7IICVY/">"Computer model aims to enhance video technology"</a> aired on WSOC-TV.</div>
    <div class="news-item"><b>Apr</b>: Serving as Area Chair for NeurIPS 2025.</div>
    <div class="news-item"><b>Feb</b>: LLAVIDAL accepted to CVPR 2025. <a href="https://inside.charlotte.edu/featured-stories/hey-siri/">[Featured Story]</a></div>
    <div class="news-item"><b>Feb</b>: 3rd-place in Elderly Action Recognition Challenge - <a href="images/ear_certificate.png">WACV 2025</a>.</div>
  </details>
  <details>
    <summary>2024</summary>
    <div class="news-item"><b>Dec</b>: 2 papers accepted to AAAI 2025.</div>
    <div class="news-item"><b>Oct</b>: 3 papers accepted to NeurIPS 2024 workshops; early version of LLAVIDAL presented at VLM workshop.</div>
    <div class="news-item"><b>Jul</b>: 2 papers accepted to ECCV 2024, 1 to ACM MM 2024.</div>
    <div class="news-item"><b>Apr</b>: DeepFake Generation paper accepted at CVPRW 2024.</div>
    <div class="news-item"><b>Feb</b>: 3 papers accepted to CVPR 2024.</div>
  </details>
  <details>
    <summary>2023</summary>
    <div class="news-item"><b>Oct</b>: Paper accepted to WACV 2024.</div>
    <div class="news-item"><b>Aug</b>: DeepFake detection paper at ICCVW 2023 and CLIP for Action Detection at BMVC 2023.</div>
    <div class="news-item"><b>Jul</b>: CMMC received <a href="images/poster_award_mva.JPG">Best Poster Award</a> at MVA 2023.</div>
    <div class="news-item"><b>May</b>: Serving as SPC at AAAI 2024 for the second time.</div>
    <div class="news-item"><b>May</b>: Dominick Reilly awarded Chateaubriand Fellowship.</div>
    <div class="news-item"><b>Feb</b>: First NSF Grant awarded – <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2245652&HistoricalAwards=false">Link</a>.</div>
    <div class="news-item"><b>Jan</b>: Paper accepted to ISBI 2023 with Stony Brook Medicine collaborators.</div>
  </details>
  <details>
    <summary>2022</summary>
    <div class="news-item"><b>Sep</b>: Two papers accepted to NeurIPS 2022.</div>
    <div class="news-item"><b>Aug</b>: Paper accepted to WACV 2023 (first round).</div>
    <div class="news-item"><b>Aug</b>: Joined UNC Charlotte as Assistant Professor.</div>
  </details>
</section>


            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>

            <td style="padding:20px;width:60%;vertical-align:top">
              <heading>Lab members</heading>
                <style>
              .lab-member {
                font-size: 1em; /* change the font size to your desired size */
                text-decoration: underline; /* add an underline */
                }
            </style>
              <p style="margin-left: 40px">
              <ul>
            <subheading class="lab-member">Current PhD Students</subheading>

                <li><em><a href="https://dominickrei.github.io/">Dominick Reilly</a>  </em>
              <li><em><a href="https://thearkaprava.github.io/">Arkaprava Sinha</a>  </em>
              <li><em><a href="https://manishgovind.github.io/">Manish Kumar Govind</a> (co-supervised with <a href="https://webpages.charlotte.edu/pwang13/"> Prof. Pu Wang </a>) </em>
              <li><em><a href="https://westonbond.github.io/website/">Weston Bondurant</a> (co-supervised with <a href="https://cci.charlotte.edu/people/stephanie-schuckers/"> Prof. Stephanie Schuckers </a>) </em>
              <li><em>Wenhao Chi</em></li>
              </ul>
               </p>
                <p style="margin-left: 40px">
              <ul>
                <subheading class="lab-member">Other Current Students</subheading>
                <li><em>Nitin Chandrasekhar (UG student at UNC Charlotte) </em>
                <li><em>Drew O’Donnell (UG student at UNC Charlotte) </em>


              </ul>
               </p>
              <!-- This is a comment
                <p style="margin-left: 40px">
              <ul>
                <subheading class="lab-member">Past Students</subheading>
                    <li><em><a href="https://monishsoundarraj.com/">Monish Soundar Raj </a> (UG student at UNC Charlotte) </em>
                    <li><em>Jack Douglass (UG student at UNC Charlotte) </em>
                    <li><em>Vishal Bondili, Jonathan Lorray (Master student at UNC Charlotte) </em>
                    <li><em>Jacob Nielsen (jointly supervised with Prof. Aritra Dutta at SDU)</em>
                    <li><em>Ian Boyle, <a href="data/URC_Naveen.pdf">Naveen Vellaturi</a>, <a href="data/URC_Sindhu.pdf">Sindhu Gadiraju</a> (UG at UNC Charlotte) </em>
                    <li><em>Tanmay Jain (intern from DTU, India), 2022-23 </em>
                    <li><em>Soumyajit Karmakar, Shyam Marjit (intern from IIIT Guwahati, India) </em>
              </p>
              -->
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><p>
        <heading>Selected Publications</heading> (For full list of papers, visit my <a href="https://scholar.google.com/citations?user=ZDTF5AEAAAAJ">Google Scholar</a>.) <br>
          <tr>
          <td>
            <style>
              .pub-year {
                font-size: 2em; /* change the font size to your desired size */
                text-decoration: underline; /* add an underline */
                }
            </style>
            <subheading class="pub-year">Preprints</subheading>
          </td>
          </tr>
        <p>

          <tr onmouseout="egoexo_stop()" onmouseover="egoexo_start()" >
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/egoexo_new.png' height=150px width=300px>
              </div>
              <script type="text/javascript">
                function egoexo_start() {
                  document.getElementById('egoexo').style.opacity = "1";
                }

                function egoexo_stop() {
                  document.getElementById('egoexo_image').style.opacity = "0";
                }
                egoexo_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2501.05711">
                <papertitle>From My View to Yours: Ego-Augmented Learning in Large Vision Language Models for Understanding Exocentric Daily Living Activities</papertitle>
              </a>
              <br>
              Dominick Reilly, Manish Kumar Govind, Le Xue, and <strong>Srijan Das</strong>.
              <br>
							<em>

                                <strong>Preprint</strong></em>
              <br>
              <a href="https://arxiv.org/pdf/2501.05711">arXiv</a>
              /
              <a href="https://github.com/dominickrei/EgoExo4ADL">Code</a>
              <p></p>
              <p>
                 We leverage the complementary nature of egocentric views to enhance LVLM’s understanding of exocentric ADL videos through online ego2exo distillation. </td>
          </tr>

          <tr onmouseout="mstemba_stop()" onmouseover="mstemba_start()">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/MS_Temba_new.png' height=150px width=300px>
              </div>
              <script type="text/javascript">
                function mstemba_start() {
                  document.getElementById('mstemba').style.opacity = "1";
                }

                function mstemba_stop() {
                  document.getElementById('mstemba_image').style.opacity = "0";
                }
                mstemba_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2501.06138">
                <papertitle>MS-Temba : Multi-Scale Temporal Mamba for Efficient Temporal Action Detection </papertitle>
              </a>
              <br>
              Arkaprava Sinha, Monish Soundar Raj, Pu Wang, Ahmed Helmy, and  <strong>Srijan Das</strong>.
              <br>
							<em>

                                <strong>Preprint</strong></em>
              <br>
              <a href="https://arxiv.org/abs/2501.06138">arXiv</a>
                /
              <a href="https://github.com/thearkaprava/MS-Temba">code</a>
              <p></p>
              <p>
                MS-Temba is the first Mamba based architecture for action detection in long untrimmed videos that can be trained/tested on NVIDIA Jetson Nano.
            </td>
          </tr>

          <td>
          <br><subheading class="pub-year">2025</subheading>
        </td>

          <tr onmouseout="gecko_stop()" onmouseover="gecko_start()" >
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/gecko.png' height=150px width=300px>
              </div>
              <script type="text/javascript">
                function gecko_start() {
                  document.getElementById('gecko').style.opacity = "1";
                }

                function genhmr_stop() {
                  document.getElementById('gecko_image').style.opacity = "0";
                }
                gecko_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2504.01009">
                <papertitle>GECKO: Gigapixel Vision-Concept Contrastive Pretraining in Histopathology</papertitle>
              </a>
              <br>
              Saarthak Kapse, Pushpak Pati, Srikar Yellapragada, <strong> Srijan Das </strong>, Rajarsi R. Gupta, Joel Saltz, Dimitris Samaras, Prateek Prasanna.
              <br>
							<em>

                                <strong>To Appear in ICCV 2025</strong></em>
              <br>
              <a href="https://arxiv.org/abs/2504.01009">arXiv</a>
              /
              <a href="https://github.com/bmi-imaginelab/GECKO?tab=readme-ov-file">Code</a>
              <p></p>
              <p>
                Gigapixel Vision-Concept Knowledge Contrastive pretraining (GECKO) aligns WSIs with a Concept Prior for delivering clinically meaningful interpretability. </td>
          </tr>

          <tr onmouseout="gmmhmr_stop()" onmouseover="gmmhmr_start()" >
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/gmmhmr.gif' height=150px width=300px>
              </div>
              <script type="text/javascript">
                function gmmhmr_start() {
                  document.getElementById('gmmhmr').style.opacity = "1";
                }

                function genhmr_stop() {
                  document.getElementById('gmmhmr_image').style.opacity = "0";
                }
                gmmhmr_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2412.13393">
                <papertitle>MaskHand: Generative Masked Modeling for Robust Hand Mesh Reconstruction in the Wild</papertitle>
              </a>
              <br>
              Muhammad Usama Saleem, Ekkasit Pinyoanuntapong, Mayur Jagdishbhai Patel, Hongfei Xue, Ahmed Helmy, <strong>Srijan Das</strong>, Pu Wang.
              <br>
							<em>

                                <strong>To Appear in ICCV 2025</strong></em>
              <br>
              <a href="https://arxiv.org/abs/2412.13393">arXiv</a>
              /
              <a href="https://m-usamasaleem.github.io/publication/MMHMR/mmhmr.html">Website</a>
              <p></p>
              <p>
                A novel generative masked model for hand mesh recovery that synthesizes plausible 3D hand meshes. </td>
          </tr>

          <tr onmouseout="llavidal_stop()" onmouseover="llavidal_start()" bgcolor="#ffffd0">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/adlx.jpg' height=150px width=300px>
              </div>
              <script type="text/javascript">
                function llavidal_start() {
                  document.getElementById('llavidal').style.opacity = "1";
                }

                function llavidal_stop() {
                  document.getElementById('llavidal_image').style.opacity = "0";
                }
                llavidal_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2406.09390">
                <papertitle>LLAVIDAL : A Large LAnguage VIsion Model for Daily Activities of Living </papertitle>
              </a>
              <br>
              Dominick Reilly, Rajatsubhra Chakraborty, Arkaprava Sinha, Manish Kumar Govind, Pu Wang, Francois Bremond, Le Xue, <strong>Srijan Das</strong>.
              <br>
							<em>

                                <strong>CVPR 2025</strong></em>
              <br>
              <a href="https://arxiv.org/abs/2406.09390">arXiv</a>
                /
              <a href="https://adl-x.github.io/">website</a>
              /
              <a href="https://github.com/ADL-X/LLAVIDAL">code</a>
              <p></p>
              <p>
                LLAVIDAL, a Large Language Vision Model, incorporates 3D poses and relevant object trajectories to understand the intricate spatiotemporal relationships within ADLs.
            </td>
          </tr>

          <tr onmouseout="skimodels_stop()" onmouseover="skimodels_start()" >
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/ski_models.png' height=150px width=300px>
              </div>
              <script type="text/javascript">
                function skimodels_start() {
                  document.getElementById('skimodels').style.opacity = "1";
                }

                function skimodels_stop() {
                  document.getElementById('skimodels_image').style.opacity = "0";
                }
                skimodels_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2502.03459">
                <papertitle>SKI Models: SKeleton Induced Vision-Language Embeddings for Understanding Activities of Daily Living</papertitle>
              </a>
              <br>
              Arkaprava Sinha, Dominick Reilly, Francois Bremond, Pu Wang, and <strong>Srijan Das</strong>.
              <br>
							<em>

                                <strong>AAAI 2025</strong></em>
              <br>
              <a href="https://arxiv.org/pdf/2502.03459">arXiv</a>
              /
              <a href="https://github.com/thearkaprava/SKI-Models">Code</a>
              <p></p>
              <p>
                Ski-models introduce 3D skeletons into the vision-language embedding space to enable effective zeroshot learning for ADL. </td>
          </tr>

          <tr onmouseout="genhmr_stop()" onmouseover="genhmr_start()" >
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/genhmr.gif' height=150px width=300px>
              </div>
              <script type="text/javascript">
                function genhmr_start() {
                  document.getElementById('genhmr').style.opacity = "1";
                }

                function genhmr_stop() {
                  document.getElementById('genhmr_image').style.opacity = "0";
                }
                genhmr_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2412.14444">
                <papertitle>GenHMR: Generative Human Mesh Recovery</papertitle>
              </a>
              <br>
              Muhammad Usama Saleem , Ekkasit Pinyoanuntapong, Pu Wang, Hongfei Xue, <strong>Srijan Das</strong>, Chen Chen.
              <br>
							<em>

                                <strong>AAAI 2025</strong></em>
              <br>
              <a href="https://arxiv.org/abs/2412.14444">arXiv</a>
              /
              <a href="https://m-usamasaleem.github.io/publication/GenHMR/GenHMR.html">Website</a>
              <p></p>
              <p>
                A generative framework that reformulates monocular HMR as an image-conditioned generative task.
            </td>
          </tr>

          <td>
          <br><subheading class="pub-year">2024</subheading>
        </td>
          <tr onmouseout="freq_stop()" onmouseover="freq_start()" >
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/FreqMixformer.png' height=150px width=300px>
              </div>
              <script type="text/javascript">
                function freq_start() {
                  document.getElementById('freq').style.opacity = "1";
                }

                function freq_stop() {
                  document.getElementById('freq_image').style.opacity = "0";
                }
                freq_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2407.12322">
                <papertitle>Frequency Guidance Matters: Skeletal Action Recognition by Frequency-Aware Mixed Transformer</papertitle>
              </a>
              <br>
              Wenhan Wu, Ce Zheng, Zihao Yang, Chen Chen, <strong>Srijan Das</strong>, Aidong Lu.
              <br>
							<em>

                                <strong>ACM MM 2024</strong></em>
              <br>
              <a href="https://arxiv.org/abs/2407.12322">arXiv</a>
              /
              <a href="https://github.com/wenhanwu95/FreqMixFormer">code</a>
              <p></p>
              <p>
                 A frequency-aware attention module to unweave skeleton frequency representations for action recognition.
            </td>
          </tr>

          <tr onmouseout="mpmc_stop()" onmouseover="mpmc_start()" >
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/MPMC.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function mpmc_start() {
                  document.getElementById('mpmc').style.opacity = "1";
                }

                function mpmc_stop() {
                  document.getElementById('mpmc_image').style.opacity = "0";
                }
                mpmc_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2407.04036">
                <papertitle>Beyond Pixels: Semi-Supervised Semantic Segmentation with a Multi-scale Patch-based Multi-Label Classifier</papertitle>
              </a>
              <br>
              Prantik Howlader, <strong>Srijan Das</strong>, Hieu Le, Dimitris Samaras.
              <br>
							<em>

                                <strong>ECCV 2024</strong></em>
              <br>
              <a href="https://arxiv.org/abs/2407.04036">arXiv</a>
                /
              <a href="https://github.com/cvlab-stonybrook/Beyond-Pixels">code</a>
              <p></p>
              <p>
                A novel plug-in module designed for existing semi-supervised segmentation frameworks that offers patch-level supervision.
            </td>
          </tr>

          <tr onmouseout="bamm_stop()" onmouseover="bamm_start()" >
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/BAMM.gif' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function bamm_start() {
                  document.getElementById('bamm').style.opacity = "1";
                }

                function llavidal_stop() {
                  document.getElementById('bamm_image').style.opacity = "0";
                }
                bamm_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02316.pdf">
                <papertitle>BAMM: Bidirectional Autoregressive Motion Model</papertitle>
              </a>
              <br>
              Ekkasit Pinyoanuntapong, Muhammad Usama Saleem, Pu Wang, Minwoo Lee, <strong>Srijan Das</strong>, Chen Chen.
              <br>
							<em>

                                <strong>ECCV 2024</strong></em>
              <br>
              <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02316.pdf">arXiv</a>
                /
              <a href="https://exitudio.github.io/BAMM-page/">website</a>
              /
              <a href="https://github.com/exitudio/BAMM/">code</a>
              <p></p>
              <p>
                A novel text-to-motion generation framework. BAMM captures rich and bidirectional dependencies among motion tokens.
            </td>
          </tr>

          <tr onmouseout="pivit_stop()" onmouseover="pivit_start()" bgcolor="#ffffd0">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/pivit.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function pivit_start() {
                  document.getElementById('pivit').style.opacity = "1";
                }

                function pivit_stop() {
                  document.getElementById('pivit_image').style.opacity = "0";
                }
                pivit_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2311.18840">
                <papertitle>Just Add π! Pose Induced Video Transformers for Understanding Activities of Daily Living</papertitle>
              </a>
              <br>
              Dominick Reilly and <strong>Srijan Das</strong>.
              <br>


                         <em>       <strong>CVPR 2024</strong></em>
              <br>
              <a href="https://arxiv.org/abs/2311.18840">arXiv</a>
                /
              <a href="https://github.com/dominickrei/pi-vit">code</a>
              <p></p>
              <p>
						We introduce the first Pose Induced Video Transformer: PI-ViT (or π-ViT), a novel approach that augments the RGB representations learned by video transformers with 2D and 3D pose information.</p>
            </td>
          </tr>
           <tr onmouseout="simil_stop()" onmouseover="simil_start()" >
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/SI-MIL.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function pivit_start() {
                  document.getElementById('simil').style.opacity = "1";
                }

                function pivit_stop() {
                  document.getElementById('simil_image').style.opacity = "0";
                }
                simil_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2312.15010">
                <papertitle>SI-MIL: Taming Deep MIL for Self-Interpretability in Gigapixel Histopathology</papertitle>
              </a>
              <br>
              Saarthak Kapse<sup>*</sup>, Pushpak Pati<sup>*</sup>, <strong>Srijan Das</strong>, Jingwei Zhang, Chao Chen, Maria Vakalopoulou, Joel Saltz, Dimitris Samaras, Rajarsi Gupta, Prateek Prasanna.
              <br>
							<em> <strong>CVPR 2024</strong></em>
              <br>
              <a href="https://arxiv.org/abs/2312.15010">arXiv</a>
                /
              <a href="https://github.com/bmi-imaginelab/SI-MIL"> code </a>
              <p></p>
              <p>
					Self-Interpretable MIL (SI-MIL), the first interpretable-by-design MIL method for gigapixel WSIs, which provides de novo feature-level interpretations grounded on pathological insights for a WSI.</p>
            </td>
          </tr>
          <tr onmouseout="mavrec_stop()" onmouseover="mavrec_start()" >
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/mavrec.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function mavrec_start() {
                  document.getElementById('mavrec').style.opacity = "1";
                }

                function paat_stop() {
                  document.getElementById('mavrec_image').style.opacity = "0";
                }
                mavrec_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2312.04548">
                <papertitle>Multiview Aerial Visual Recognition (MAVREC): Can Multi-view Improve Aerial Visual Perception?</papertitle>
              </a>
              <br>
              Aritra Dutta, <strong> Srijan Das </strong>, Jacob Nielsen, Rajatsubhra Chakraborty, Mubarak Shah.
              <br>
						<em>	<strong>CVPR 2024</strong></em>
              <br>
              <a href="https://arxiv.org/abs/2312.04548">arXiv</a>
                /
              <a href="https://mavrec.github.io/">Website</a>
              <p></p>
              <p>
              We present MAVREC, a video dataset where we record synchronized scenes from different perspectives -- ground camera and drone-mounted camera. </p>
            </td>
          </tr>
          <tr onmouseout="dirl_stop()" onmouseover="dirl_start()">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/dirl.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function dirl_start() {
                  document.getElementById('DIRL').style.opacity = "1";
                }
                function dirl_stop() {
                  document.getElementById('DIRL').style.opacity = "0";
                }
                dirl_stop()
              </script>
           </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <a href="https://arxiv.org/abs/2309.06439">
                     <papertitle>Attention de-sparsification Matters: Inducing Diversity in Digital Pathology Representation Learning</papertitle>
               </a>
              <br>
              Saarthak Kapse, <strong>Srijan Das</strong>, Jingwei Zhang, Rajarsi R. Gupta, Joel Saltz, Dimitris Samaras, Prateek Prasanna.
              <br>
              <em> <strong> Medical Image Analysis (IF 10.9) </strong></em>
              <br>
              <a href="https://arxiv.org/abs/2309.06439">arXiv</a>
              <p> A diversity-inducing pretraining technique, tailored to enhance representation learning in digital pathology.</p>
            </td>
          </tr>
            <tr onmouseout="smalltr_stop()" onmouseover="smalltr_start()" bgcolor="#ffffd0">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/wacv24.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function smalltr_start() {
                  document.getElementById('smalltr').style.opacity = "1";
                }

                function paat_stop() {
                  document.getElementById('smalltr_image').style.opacity = "0";
                }
                smalltr_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2310.20704">
                <papertitle>Limited Data, Unlimited Potential: A Study on ViTs Augmented by Masked Autoencoders</papertitle>
              </a>
              <br>
              <strong>Srijan Das</strong>, Tanmay Jain, Dominick Reilly, Pranav Balaji, Soumyajit Karmakar, Shyam Marjit, Xiang Li, Abhijit Das, and Michael S. Ryoo.
              <br>
							<em><strong>WACV 2024</strong></em>
              <br>
              <a href="http://arxiv.org/abs/2310.20704">arXiv</a>
                /
              <a href="https://github.com/dominickrei/Limited-data-vits/">code</a>
              /
              <a href="images/wacv24_poster.pdf">Poster</a>
              /
              <a href="https://www.youtube.com/watch?v=yRBuBbUggA0&t=136s">Video</a>
              <p></p>
              <p>
						This paper shows that jointly optimizing ViTs for the primary task and a Self-Supervised Auxiliary Task is surprisingly beneficial when the amount of training data is limited.</p>
            </td>
          </tr>
          <tr>
        <td>
          <br><subheading class="pub-year">2023</subheading>
        </td>
        </tr>
            <tr onmouseout="clipad_stop()" onmouseover="clipad_start()">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/BMVC_AAN.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function clipad_start() {
                  document.getElementById('clipad').style.opacity = "1";
                }

                function paat_stop() {
                  document.getElementById('clipad_image').style.opacity = "0";
                }
                clipad_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2309.00696.pdf">
                <papertitle>Attributes-Aware Network for Temporal Action Detection</papertitle>
              </a>
              <br>
              Rui Dai, <strong>Srijan Das</strong>, Michael S. Ryoo, Francois Bremond.
              <br>
                <em><strong>BMVC 2023</strong></em>
              <br>
              <a href="https://arxiv.org/pdf/2309.00696.pdf"> arXiv </a> / <a href="https://youtu.be/PMj-UEZLXzg">video</a>
              <p></p>
              <p>
					This paper explains how to utilize OpenAI's CLIP for long-term action detection in videos.		</p>
            </td>
          </tr>
            <tr onmouseout="stc_stop()" onmouseover="stc_start()">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/stc-mix.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function stc20_start() {
                  document.getElementById('wstc_image').style.opacity = "1";
                }
                function stc_stop() {
                  document.getElementById('stc_image').style.opacity = "0";
                }
                stc_stop()
              </script>
           </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <a href="https://arxiv.org/abs/2112.03906">
                     <papertitle>Cross-modal Manifold Cutmix for Self-supervised Video Representation Learning</papertitle>
               </a>
              <br>
              <strong>Srijan Das</strong> and Michael S. Ryoo.
              <br>
							<em>18th International Conference on Machine Vision Applications </em>, July 2023
              <br>
                <a href="https://arxiv.org/abs/2112.03906">arXiv</a>
                /
                <a href="images/MVA23_poster.pdf"> Poster </a>
                /
                <a href="images/poster_award_mva.JPG" style="color: red; font-weight: bold;">Best Poster Award</a>
              <p> This paper focuses on designing video augmentation for self-supervised learning, we propose CMMC to make use of other modalities in videos for data mixing.</p>
            </td>
          </tr>

        <tr onmouseout="viewclr_stop()" onmouseover="viewclr_start()" bgcolor="#ffffd0">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/viewclr.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function viewclr_start() {
                  document.getElementById('viewclr_image').style.opacity = "1";
                }

                function rawnerf_stop() {
                  document.getElementById('viewclr_image').style.opacity = "0";
                }
                viewclr_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2112.03905">
                <papertitle>ViewCLR: Learning Self-supervised Video Representation for Unseen Viewpoints</papertitle>
              </a>
              <br>
              <strong>Srijan Das</strong>, and Michael S. Ryoo.
              <br>
							<em><strong>WACV 2023</strong></em>
              <br>
              <a href="https://arxiv.org/abs/2112.03905">arXiv</a>
              <p></p>
              <p>
								A framework for learning self-supervised video representation that is invariant to unseen camera viewpoints.</p>
            </td>
          </tr>

          <tr>
        <td>
          <br><subheading class="pub-year">2022</subheading>
        </td>
        </tr>

          <tr onmouseout="xiangssl_stop()" onmouseover="xiangssl_start()">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/Xiang_SSL.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function xiangssl_start() {
                  document.getElementById('Xiang_SSL').style.opacity = "1";
                }
                function xiangssl_stop() {
                  document.getElementById('Xing_SSL').style.opacity = "0";
                }
                xiangssl_stop()
              </script>
           </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <a href="https://arxiv.org/abs/2206.05266">
                     <papertitle>Does Self-supervised Learning Really Improve Reinforcement Learning from Pixels?</papertitle>
               </a>
              <br>
              Xiang Li, Jinghuan Shang, <strong>Srijan Das</strong>, Michael S. Ryoo.
              <br>
              <em><strong>NeurIPS 2022</strong></em>
              <br>
              <a href="https://arxiv.org/abs/2206.05266">arXiv</a>
                /
              <a href="https://github.com/LostXine/elo-sac">code</a>
              <p> The impacts of the existing self-supervised losses with  Joint Learning framework for RL is limited, while there is no golden method that can dominate all tasks. </p>
            </td>
          </tr>

          <tr onmouseout="trl_stop()" onmouseover="trl_start()">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/3dtrl.gif' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function trl_start() {
                  document.getElementById('TRL').style.opacity = "1";
                }
                function trl_stop() {
                  document.getElementById('TRL').style.opacity = "0";
                }
                trl_stop()
              </script>
           </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <a href="https://arxiv.org/pdf/2206.11895.pdf">
                     <papertitle>Learning Viewpoint-Agnostic Visual Representations by Recovering Tokens in 3D Space</papertitle>
               </a>
              <br>
              Jinghuan Shang, <strong>Srijan Das</strong>, Michael S. Ryoo.
              <br>
              <em><strong>NeurIPS 2022</strong></em>
              <br>
              <a href="https://arxiv.org/pdf/2206.11895.pdf">arXiv</a>
               /
              <a href="https://www3.cs.stonybrook.edu/~jishang/3dtrl/3dtrl.html">Project Page</a>
                /
              <a href="https://github.com/elicassion/3DTRL">code</a>
              <p> 3DTRL is a light-weighted, plug-and play layer that recovers 3D information of visual tokens and leverages it for learning viewpoint-agnostic representations.</p>
            </td>
          </tr>

          <tr onmouseout="tsu_stop()" onmouseover="tsu_start()" bgcolor="#ffffd0">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='tsu_image'><img src='imagesTSU/TSU_fig.png' height=175px width=250px></div>
                <img src='images/TSU_fig.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function tsu_start() {
                  document.getElementById('tsu_image').style.opacity = "1";
                }
                function tsu_stop() {
                  document.getElementById('tsu_image').style.opacity = "0";
                }
                tsu_stop()
              </script>
           </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <a href="https://arxiv.org/abs/2010.14982">
                     <papertitle>Toyota Smarthome Untrimmed: Real-World Untrimmed Videos for Activity Detection</papertitle>
               </a>
              <br>
              Rui Dai, <strong>Srijan Das</strong>, Saurav Sharma, Luca Minciullo, Lorenzo Garattoni, François Brémond, Gianpiero Francesca.
              <br>
              <em><strong>T-PAMI 2022</strong></em>
              <br>
              <br>
              <a href="https://project.inria.fr/toyotasmarthome/">Project Link</a> / <a href="https://github.com/dairui01/TSU_evaluation">Code</a>
              <p> TSU is a new untrimmed daily-living dataset consisting of 51 activities performed in a spontaneous manner, captured from non-optimal viewpoints.</p>
            </td>
          </tr>

        <tr onmouseout="mstct_stop()" onmouseover="mstct_start()">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/ms-tct.gif' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function mstct_start() {
                  document.getElementById('MSTCT').style.opacity = "1";
                }
                function mstct_stop() {
                  document.getElementById('MSTCT').style.opacity = "0";
                }
                mstct_stop()
              </script>
           </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <a href="https://arxiv.org/pdf/2112.03902.pdf">
                     <papertitle>MS-TCT: Multi-Scale Temporal ConvTransformer for Action Detection</papertitle>
               </a>
              <br>
              Rui Dai, <strong>Srijan Das</strong>, Kumara Kahatapitiya, Michael S. Ryoo, Francois Bremond.
              <br>
              <em><strong>CVPR 2022</strong></em>
              <br>
              <a href="https://arxiv.org/abs/2112.03902">arXiv</a>
               /
              <a href="https://github.com/dairui01/MS-TCT">code</a>
              <p> A ConvTransformer network that explores global and local temporal relations at multiple resolutions.</p>
            </td>
          </tr>
          <tr>
        <td>
          <br><subheading class="pub-year">2021</subheading>
        </td>
        </tr>

          <tr onmouseout="vpn++_stop()" onmouseover="vpn++_start()"  bgcolor="#ffffd0">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">

                <img src='images/vpn++.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function mip360_start() {
                  document.getElementById('vpn++_image').style.opacity = "1";
                }

                function mip360_stop() {
                  document.getElementById('vpn++_image').style.opacity = "0";
                }
                vpn++_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2105.08141">
                <papertitle>VPN++: Rethinking Video-Pose embeddings for understanding Activities of Daily Living</papertitle>
              </a>
              <br>
              <strong>Srijan Das</strong>, Rui Dai, Di Yang, Francois Bremond,
              <br>
							<em><strong>TPAMI</strong></em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2105.08141">arXiv</a>
              /
              <a href="https://github.com/srijandas07/vpnplusplus">code</a>
              <p></p>
              <p>VPN++ is an extension of our VPN model (ECCV 2020). VPN++ hallucinates pose driven features while not requiring costly 3D Poses at inference.</p>
            </td>
          </tr> 

          <tr onmouseout="bmvc21_stop()" onmouseover="bmvc21_start()">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/ctrn.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function bmvc21_start() {
                  document.getElementById('bmvc21_image').style.opacity = "1";
                }
                function bmvc21_stop() {
                  document.getElementById('bmvc21_image').style.opacity = "0";
                }
                tsu_stop()
              </script>
           </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <a href="https://www.bmvc2021-virtualconference.com/conference/papers/paper_0133.html">
                     <papertitle>CTRN: Class Temporal Relational Network for Action Detection</papertitle>
               </a>
              <br>
              Rui Dai, <strong>Srijan Das</strong>, Francois Bremond.
              <br>
              <em> <!--The British Machine Vision Conference, 2021. -->
                <strong>BMVC 2021, Oral</strong></em>
              <br>
            </td>
          </tr>

          <tr onmouseout="iccv21_stop()" onmouseover="iccv21_start()">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/dist_actdet.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function iccv21_start() {
                  document.getElementById('vpnp_image').style.opacity = "1";
                }
                function iccv21_stop() {
                  document.getElementById('vpnp_image').style.opacity = "0";
                }
                tsu_stop()
              </script>
           </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Dai_Learning_an_Augmented_RGB_Representation_With_Cross-Modal_Knowledge_Distillation_for_ICCV_2021_paper.html">
                     <papertitle>Learning an Augmented RGB Representation with Cross-Modal Knowledge Distillation for Action Detection</papertitle>
               </a>
              <br>
              Rui Dai, <strong>Srijan Das</strong>, Francois Bremond.
              <br>
              <em> <!--IEEE/CVF International Conference on Computer Vision, 2021. -->
                <strong>ICCV 2021</strong>
              <br>
            </td>
          </tr>

          <tr onmouseout="pdan_stop()" onmouseover="pdan_start()">
            <td style="padding:70px;width:25%;vertical-align:middle"
              <div class="one">
                <img src='images/pdan.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function pdan_start() {
                  document.getElementById('pdan_image').style.opacity = "1";
                }
                function pdan_stop() {
                  document.getElementById('pdan_image').style.opacity = "0";
                }
                vpn_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/WACV2021/papers/Dai_PDAN_Pyramid_Dilated_Attention_Network_for_Action_Detection_WACV_2021_paper.pdf">
                <papertitle>PDAN: Pyramid Dilated Attention Network for Action Detection.</papertitle>
              </a>
              <br>
              Rui Dai, <strong>Srijan Das</strong>, Luca Minciullo, Lorenzo Garattoni, Gianpiero Francesca and Francois Bremond.
              <br>
              <em><!--IEEE Winter Conference on Applications of Computer Vision</em>, 2021. -->
                <strong>WACV 2021</strong>
              <br>
              <a href="https://github.com/dairui01/PDAN">Code</a> / <a href="https://www.youtube.com/watch?v=lvEcjufIkdo">Video</a> / <a href="research/RLDL_poster.pdf">Poster</a>
            </td>
          </tr>
          <tr>
        <td>
          <br><subheading class="pub-year">2020</subheading>
        </td>
        </tr>
  <tr onmouseout="vpn_stop()" onmouseover="vpn_start()" bgcolor="#ffffd0">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/vpn.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function vpn_start() {
                  document.getElementById('vpn_image').style.opacity = "1";
                }
                function vpn_stop() {
                  document.getElementById('vpn_image').style.opacity = "0";
                }
                vpn_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2007.03056">
                <papertitle>VPN: Learning Video-Pose Embedding for Activities of Daily Living</papertitle>
              </a>
              <br>
              <strong>Srijan Das</strong>, Saurav Sharma, Rui Dai, Francois Bremond, Monique Thonnat.
              <br>
              <em><!--16th European Conference on Computer Vision</em>, 2020.-->
                <strong>ECCV 2020</strong>
              <br>
              <br>
              <a href="https://github.com/srijandas07/VPN">Code</a>
            </td>
          </tr>

          <tr onmouseout="wacv20_stop()" onmouseover="wacv20_start()">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/temporal_model.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function wacv20_start() {
                  document.getElementById('wacv20_image').style.opacity = "1";
                }
                function wacv20_stop() {
                  document.getElementById('wacv20_image').style.opacity = "0";
                }
                tsu_stop()
              </script>
           </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <a href="https://openaccess.thecvf.com/content_WACV_2020/html/Das_Looking_deeper_into_Time_for_Activities_of_Daily_Living_Recognition_WACV_2020_paper.html">
                     <papertitle>Looking deeper into Time for Activities of Daily Living Recognition</papertitle>
               </a>
              <br>
              <strong>Srijan Das</strong>, Monique Tonnat and Francois Bremond.
              <br>
              <em> <!--Winter Conference on Applications of Computer Vision, 2020. -->
                <strong>WACV 2020</strong></em>
              <br>
            </td>
          </tr>
        <tr>
        <td>
          <br><subheading class="pub-year">2019</subheading>
        </td>
        </tr>
        <tr onmouseout="sta_stop()" onmouseover="sta_start()" bgcolor="#ffffd0">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/STA_fig.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function stego_start() {
                  document.getElementById('sta_image').style.opacity = "1";
                }

                function stego_stop() {
                  document.getElementById('sta_image').style.opacity = "0";
                }
                stego_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Das_Toyota_Smarthome_Real-World_Activities_of_Daily_Living_ICCV_2019_paper.pdf">
                <papertitle>Toyota Smarthome: Real World Activities of Daily Living.</papertitle>
              </a>
              <br>
              <strong>Srijan Das</strong>, Rui Dai, Michal Koperski, Luca Minciullo, Lorenzo Garattoni, Francois Bremond and Gianpiero Francesca.
              <br>
              <em><!--In Proceedings of the 17th International Conference on Computer Vision</em>, 2019.-->
                <strong>ICCV 2019</strong>
              <br>
              <br>
              <a href="https://project.inria.fr/toyotasmarthome/">Project Link</a> / <a href="https://github.com/DAIGroup/separable_STA">Code</a>
            </td>
          </tr>
          <tr onmouseout="wacv19_stop()" onmouseover="wacv19_start()">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/P-i3d.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function wacv19_start() {
                  document.getElementById('wacv19_image').style.opacity = "1";
                }
                function wacv19_stop() {
                  document.getElementById('wacv19_image').style.opacity = "0";
                }
                tsu_stop()
              </script>
           </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <a href="https://ieeexplore.ieee.org/document/8658564">
                     <papertitle>Where to focus on for Human Action Recognition?</papertitle>
               </a>
              <br>
              <strong>Srijan Das</strong>, Arpit Chaudhary, Francois Bremond and Monique Thonnat.
              <br>
              <em> <!--Winter Conference on Applications of Computer Vision, 2019. -->
                <strong>WACV 2019</strong></em>
              <br>
            </td>
          </tr>
        </p>



          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Datasets</heading>
              <p style="margin-left: 40px">
              <ul>
                <li><b><em> <a href="https://project.inria.fr/toyotasmarthome/">Toyota Smarthome Trimmed (2019)</a> </em></b>   </li>
                <li><b><em><a href="https://project.inria.fr/toyotasmarthome/">Toyota Smarthome Untrimmed (2021)</a></em></b>  </li>
                <li><b><em><a href="https://mavrec.github.io/">MAVREC Dataset (2023)</a></em></b> &nbsp;&nbsp; </li>
              <li><b><em><a href="https://adl-x.github.io/">ADL-X Dataset (2024)</a></em></b> &nbsp;&nbsp; </li>
              </ul>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Talks</heading>
              <p style="margin-left: 40px">
              <ul>
                <li><b><em>May 2025</em></b> &nbsp;&nbsp; Invited Talk on "LLAVIDAL: A Large LAnguage VIsion Model for Daily Activities of Living" in the first special WACV 2025 Meetup Series.
                <li><b><em>May 2025</em></b> &nbsp;&nbsp; Invited Academic Talk on "Improved Reasoning in AI Models for Deepfake Detection" in Martigny Biometrics Workshop co-organised by the European Association for Biometrics (EAB), the Center for Identification Technology Research (CITeR) and the Idiap Research Institute at Idiap in Martigny, Switzerland.
                <li><b><em>May 2025</em></b> &nbsp;&nbsp; Invited research poster presentation at the Computing Community Consortium (CCC) Computing Futures Symposium in Washington, DC, USA.
                <li><b><em>Mar 2025</em></b> &nbsp;&nbsp; Guest Lecture on "Deep Neural Networks" at The University of Michigan-Dearborn.
                <li><b><em>Mar 2024</em></b> &nbsp;&nbsp; Talk on "Computer Vision Projects in CharMLab" in a RoundTable discussion on AI in conjunction with the Defense Alliance of NC (DANC) and the Michael Best Law Firm.
                <li><b><em>Feb 2024</em></b> &nbsp;&nbsp; Invited Online Tech Talk on "From Pixels to Robots: Recipes for Vision-Enabled Robot Learning" at Christ University, Bangalore, India.
                <li><b><em>Dec 2023</em></b> &nbsp;&nbsp; Invited Talk on "Video Understanding using AI" as part of the "AI and ROS for Robotics: Theory and Practice" short-term training program at IIITDM.
                <li><b><em>Jun 2023</em></b> &nbsp;&nbsp; Invited Talk on "Computer Vision for Robot Learning" as part of the "AI and Machine Vision for Robotics" short-term training program at IIITDM. (Virtually)  </li>
                <li><b><em>Apr 2023</em></b> &nbsp;&nbsp; Talk on "From Few to More: Enhancing ViT Performance on Limited Data" at PHPC Lab in UNC Charlotte.  </li>
                <li><b><em>Mar 2023</em></b> &nbsp;&nbsp; Talk on "From Pixels to Robots: Recipes for Vision-Enabled Robot Learning" at the Seminar on Controls and Robotics in UNC Charlotte.  </li>
                <li><b><em>Jan 2023</em></b> &nbsp;&nbsp; Talk on "Quo vadis, computer vision!" at the PhD seminar in UNC Charlotte.  </li>
                <li><b><em>Mar 2022</em></b> &nbsp;&nbsp; Invited Talk in AICTE sponsored Short Term Course on "Multiple Modalities are all you need for Video Understanding!" at IIITDM Kancheepuram. (Virtually)  </li>
                <li><b><em>Sep 2021</em></b> &nbsp;&nbsp; Talk on "Vision for understanding Activities of Daily Living" at <a href="https://www.linkedin.com/company/scitech-talks/" color="blue">SciTech Talks </a>. <a href="https://www.youtube.com/watch?v=YNFUMQkWBSk" color="blue">[video] </a> </li>
                <li><b><em>Apr 2021</em></b> &nbsp;&nbsp; Seminar talk on "How to combine modalities for understanding Activities of Daily Living? " for CSE 600 at Stony Brook University, NY, USA. </li>
                <li><b><em>Nov 2020</em></b> &nbsp;&nbsp; Seminar talk on "How to combine RGB & Poses for understanding Activities of Daily Living?" at Université Lumière Lyon 2. </li>
                <li><b><em>Nov 2019</em></b> &nbsp;&nbsp; <a href="https://www.meetup.com/fr-FR/Data-Science-Meetup-Nice-Sophia-Antipolis/events/266551193/" color="blue">Nice Data Science meetup </a>. <a href="https://drive.google.com/file/d/1mJe3Vh75wdZojeS3dxLPNOYWUIyi0c5O/view" color="blue">[slides] </a></li>
                <li><b><em>Aug 2018</em></b> &nbsp;&nbsp; <a href="http://www.innovation-alzheimer.fr/summer-school-program-2018/" color="blue">Summer School Brain Innovation Generation @ UCA </a>.  <a href="https://www.dropbox.com/s/ci4m4o2cqkpt85e/UCA_summer_school.pdf?dl=0" color="blue">[slides] </a></li>
              </ul>
              </p>
            </td>
          </tr>
        </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Teaching</heading>
                <p style="margin-left: 40px">
              <ul>

                <li><b><em>Spring 2025, Fall 2025</em></b> <a href="data/4152_Syllabus.pdf">ITCS 4152 Introduction to Computer Vision</a> </li>
              <li><b><em>Fall 2024</em></b> <a href="data/6010_8010_Syllabus.pdf">ITCS 6010/8010 Advanced Computer Vision (Topics Course)</a> </li>
                <li><b><em>Fall 2022, Spring 2023, Fall 2023, Spring 2024</em></b> <a href="data/4152_5152_Syllabus.pdf">ITCS 4152/5152 Computer Vision</a> </li>

                <li><b><em>Aug 2021</em></b> &nbsp;&nbsp; Surviving the Deep Learning Apocalypse (SKFGI Webinar series 2020) </li>
                <p style="margin-left: 80px">
              <ol type="1">
              <li>  Video Understanding: How to model Time? <a href="https://www.youtube.com/watch?v=bNINKn6S3Gs" color="blue"> [video]</a> </li>
              <li>  Tips to attend attention! <a href="https://www.youtube.com/watch?v=iBx9EvCdAC4" color="blue"> [video]</a> </li>
            </ol>
              <p>
                <li><b><em>Jan 2021</em></b> &nbsp;&nbsp; <a href="http://www-sop.inria.fr/members/Francois.Bremond/MSclass/deepLearningWinterSchool/UCA_master/index.html" color="blue">Deep Learning Winter School for Computer Vision 2019-20 </li> </p>
                <p style="margin-left: 80px">
              <ol type="1">
              <li>  Introduction to video classification & RNN. <a href="http://www-sop.inria.fr/members/Francois.Bremond/MSclass/deepLearningWinterSchool/slides/Lecture_Srijan.pdf" color="blue">[slides] </a> <a href="https://drive.google.com/file/d/1c4z9lAXdkqf1Ak7SHPtAWYwpg0hiVCQ8/view" color="blue">[assignment1] </a> <a href="https://colab.research.google.com/drive/1vZ8jaj5pMlHNnjF4KkBWf_7-lDpvxBeW" color="blue">[Practical 1]</a>  </li>
              <li>  Action Classification in videos <a href="http://www-sop.inria.fr/members/Francois.Bremond/MSclass/deepLearningWinterSchool/slides/Lecture_2.pdf" color="blue">[slides] </a> </li>
              <li>  Attention Mechanisms for video analytics <a href="http://www-sop.inria.fr/members/Francois.Bremond/MSclass/deepLearningWinterSchool/slides/Lecture_3.pdf" color="blue">[slides] </a> <a href="https://www.dropbox.com/s/miptig2m0qkbytg/Assignment_teaching.pdf?dl=0" color="blue">[assignment2] </a> <a href="http://www-sop.inria.fr/members/Francois.Bremond/MSclass/deepLearningWinterSchool/slides/Practical.pdf" color="blue">[Practical 2]</a> </li>
                </ol>
            </ul>
              </p>
            </td>
          </tr>
        </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <section class="section">
              <heading>Academic Activities</heading>
              <p style="margin-left: 40px">
              <ul>
                <li>Area Chair for NeurIPS 2025.</li>
                <li>Program committee member of AAAI-24 Student Program.</li>
                <li>Associate Editor for ICRA 2024.</li>
                <li>Member of DEI committe for CVPR 2023.</li>
                <li>Senior Program Committee Member for AAAI 2023 and AAAI 2024.</li>
                <li>Session chair for Image Understanding & Activity Recognition session at IPAS 2020. </li>
                <li>Mentored for B.E.N.J.I. in GirlScript Summer of Code 2019 edition. </li>
                <li>Mentor for the Emerging Technology Business Incubator (ETBI) Led by NIT Rourkela, a platform envisaged to transform the start-up ecosystem of the region. </li>
                <li>Reviewer at ICACIE 2017, 2018, SETIT 2018, KCST 2019, ICAML 2019, AVSS 2019, 2022, WACV 2020, 2021, 2022, CVPR 2021, 2022, 2023, 2024, 2025, ECCV 2022, 2024, ICCV 2021, 2023, 2025, AAAI 2023, NeurIPS 2023, IROS 2021, 2024.</li>
                <li>Reviewer at TPAMI, Pattern Recognition, Elsevier Journal of CVIU, Elsevier Journal of FGCS, Elsevier Journal of Computer & Electrical Engineering, MTAP, and Journal of Signal Processing: Image Communication. </li>
                <li>Volunteer at ICACNI 2014, ICACNI 2016, ICCV 2019, ICLR 2020 & ICML 2020. </li>
                </ul>
              </p>
            </td>
          </tr>
        </tbody></table>



          <footer>
    © 2025 Srijan Das. Last updated: June 2025. &nbsp;&nbsp;|&nbsp;&nbsp; <a href="https://jonbarron.info/" style="color:white;">Theme Credit</a>
  </footer>

</body>

</html>
