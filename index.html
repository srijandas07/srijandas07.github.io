<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Srijan Das</title>
  
  <meta name="author" content="Srijan Das">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Srijan Das</name>
              </p>
              <p>I am an Assistant Professor in the Department of Computer Science at the University of North Carolina at Charlotte. At UNC Charlotte, I am working on Video Representation Learning, and Robotic Vision. I am a member of the Charlotte Machine Learning Lab (<a href="https://charlotteml.github.io/">CharMLab</a>) at UNC Charlotte.
              </p>
              <p>
                  Before this, I was a Postdoctoral Associate at Stony Brook University under the supervision of <a href="http://michaelryoo.com/">Michael S. Ryoo</a>. In 2020, I completed my Ph.D. in Computer Science at INRIA, Sophia Antipolis, France under the supervision of <a href="http://www-sop.inria.fr/members/Francois.Bremond/">Francois Bremond</a> and <a href="http://www-sop.inria.fr/members/Monique.Thonnat/">Monique Thonnat</a>.
                My Ph.D. thesis is on <a href="https://hal.archives-ouvertes.fr/tel-02973812/document">¨Spatio-temporal attention mechanisms for Action Recognition¨</a> and click <a href="https://www.youtube.com/watch?v=HIsqMt9dA78">here</a> to watch my Defense Presentation.
                I did my Post-Grad in Computer Science from the <a href="https://www.nitrkl.ac.in/">National Institute of Technology (NIT), Rourkela</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:sdas24@charlotte.edu">Email</a> &nbsp/&nbsp
                <a href="data/srijan_CV_2024.pdf">CV</a> &nbsp/&nbsp
                <a href="data/srijan-bio.txt">Bio</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=ZDTF5AEAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/srijandas07">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/srijandas07/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/srijan_circle.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/srijan_circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research focuses on video representation learning, utilizing spatio-temporal attention mechanisms, multiple modalities, and both ego-exo centric viewpoints. I am also interested in vision-language models and self-supervised learning techniques. The primary applications of my research include action classification in trimmed videos, temporal action detection in untrimmed videos, video retrieval, robotic vision, and the development of video conversational agents.
              </p>
              <p>
                [<strong>Hiring</strong>]  Please find the <a href="https://docs.google.com/document/d/19VdI9u2MisJX0j5_4zDH8VdqEEFC3HlJCH2AllxtkIw/edit?usp=sharing"> instruction link </a> for potential RA positions and opportunity for students interested in conducting research with me. UNC Charlotte students asking to be my grader or TA, your emails will be ignored. Instead look for the IA application link <a href="https://cci.charlotte.edu/departments/computer-science-cs/cs-academics/masters-programs/ms-funding">here</a>.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p style="margin-left: 40px">
              <ul>
                <li><b>Jul 2024</b> - 2 papers accepted to ECCV 2024 and 1 paper to ACM MM 2024. </li>
                <li><b>Apr 2024</b> - One paper on DeepFake Generation has been accepted at CVPRW 2024. </li>
                <li><b>Feb 2024</b> - 3 papers accepted to CVPR 2024. </li>
                <li><b>Oct 2023</b> - One paper accepted to WACV 2024. </li>
                <li><b>Aug 2023</b> - One paper on DeepFake detection has been accepted at ICCVW 2023, and another paper on using CLIP for Action Detection has been accepted at BMVC 2023.</li>
                <li><b>Jul 2023</b> - CMMC received the <a href="images/poster_award_mva.JPG"> Best Poster Award </a> at MVA 2023.</li>
                <li><b>May 2023</b> - Serving as SPC at AAAI 2024 for the second time.</li>
                <li><b>May 2023</b> - Dominick has been selected as a recipient of the Chateaubriand Fellowship. Congratulations!</li>
                <li><b>Apr 2023</b> - Serving as a member of the DEI committee for CVPR 2023.</li>
                <li><b>Feb 2023</b> - First NSF Grant has been awarded. [<a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2245652&HistoricalAwards=false">Link</a>]</li>
                <li><b>Jan 2023</b> - One paper with colleagues at Stony Brook Medicine is accepted to ISBI 2023.</li>
                <li><b>Sep 2022</b> - Two papers accepted to NeurIPS 2022.</li>
                <li><b>Aug 2022</b> - One paper accepted to WACV 2023 (first round).</li>
                <li><b>Aug 2022</b> - Joined UNC Charlotte as an Asst. Professor.</li>

              </ul>
              </p>
            </td>
          </tr>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>

            <td style="padding:20px;width:60%;vertical-align:top">
              <heading>Lab members</heading>
                <style>
              .lab-member {
                font-size: 1em; /* change the font size to your desired size */
                text-decoration: underline; /* add an underline */
                }
            </style>
              <p style="margin-left: 40px">
              <ul>
            <subheading class="lab-member">Current Students</subheading>

                <li><em><a href="https://dominickrei.github.io/">Dominick Reilly</a> (PhD student at UNC Charlotte) </em>
              <li><em><a href="https://webpages.charlotte.edu/asinha13/">Arkaprava Sinha</a> (PhD student at UNC Charlotte) </em>
              <li><em> <a href="https://chakrabortyrajatsubhra.github.io/">Rajatsubhra Chakraborty</a> (PhD student at UNC Charlotte) </em>
                <li><em>Manish Kumar Govind (Master student at UNC Charlotte) </em>

              </ul>
               </p>
                <p style="margin-left: 40px">
              <ul>
                <subheading class="lab-member">Past Students</subheading>
                    <li><em>Vishal Bondili, Jonathan Lorray (Master student at UNC Charlotte) </em>
                    <li><em>Jacob Nielsen (jointly supervised with Prof. Aritra Dutta at SDU)</em>
                    <li><em>Ian Boyle, <a href="data/URC_Naveen.pdf">Naveen Vellaturi</a>, <a href="data/URC_Sindhu.pdf">Sindhu Gadiraju</a> (UG at UNC Charlotte) </em>
                    <li><em>Tanmay Jain (intern from DTU, India), 2022-23 </em>
                    <li><em>Soumyajit Karmakar, Shyam Marjit (intern from IIIT Guwahati, India) </em>
              </p>
            </td>
              <td style="width:40%;padding:20px;vertical-align:top;">
                <img src="images/team_photo.png" alt="Lab Team" style="width:100%;height:auto;">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><p>
        <heading>Selected Publications</heading> (For full list of papers, visit my <a href="https://scholar.google.com/citations?user=ZDTF5AEAAAAJ">Google Scholar</a>.) <br>
          <tr>
          <td>
            <style>
              .pub-year {
                font-size: 2em; /* change the font size to your desired size */
                text-decoration: underline; /* add an underline */
                }
            </style>
            <subheading class="pub-year">Preprint & 2024</subheading>
          </td>
          </tr>
        <p>
          <tr onmouseout="llavidal_stop()" onmouseover="llavidal_start()" bgcolor="#ffffd0">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/adlx.gif' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function llavidal_start() {
                  document.getElementById('llavidal').style.opacity = "1";
                }

                function llavidal_stop() {
                  document.getElementById('llavidal_image').style.opacity = "0";
                }
                llavidal_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2406.09390">
                <papertitle>LLAVIDAL : Benchmarking Large LAnguage VIsion Models for Daily Activities of Living</papertitle>
              </a>
              <br>
              Rajatsubhra Chakraborty*, Arkaprava Sinha*, Dominick Reilly*, Manish Kumar Govind, Pu Wang, Francois Bremond,  <strong>Srijan Das</strong>.
              <br>
							<em>

                                <strong>Preprint</strong></em>
              <br>
              <a href="https://arxiv.org/abs/2406.09390">arXiv</a>
                /
              <a href="https://adl-x.github.io/">website</a>
              /
              <a href="https://github.com/ADL-X/LLAVIDAL">code</a>
              <p></p>
              <p>
                LLAVIDAL, a Large Language Vision Model, incorporates 3D poses and relevant object trajectories to understand the intricate spatiotemporal relationships within ADLs.
            </td>
          </tr>

          <tr onmouseout="fibo_stop()" onmouseover="fibo_start()" >
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/fibo_image.png' height=150px width=300px>
              </div>
              <script type="text/javascript">
                function fibo_start() {
                  document.getElementById('fibo').style.opacity = "1";
                }

                function llavidal_stop() {
                  document.getElementById('fibo_image').style.opacity = "0";
                }
                fibo_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2406.19391">
                <papertitle>Fibottention: Inceptive Visual Representation Learning with Diverse Attention Across Heads</papertitle>
              </a>
              <br>
              Ali Khaleghi Rahimian, Manish Kumar Govind, Subhajit Maity, Dominick Reilly, Christian Kümmerle*, <strong>Srijan Das*</strong>, Aritra Dutta*.
              <br>
							<em>

                                <strong>Preprint</strong></em>
              <br>
              <a href="https://arxiv.org/abs/2406.19391">arXiv</a>
              /
              <a href="https://github.com/Charlotte-CharMLab/Fibottention/">code</a>
              <p></p>
              <p>
                Fibottention is a general, efficient, sparse architecture, for approximating self-attention with superlinear complexity that is built upon Fibonacci sequences.
            </td>
          </tr>

          <tr onmouseout="mpmc_stop()" onmouseover="mpmc_start()" >
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/MPMC.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function mpmc_start() {
                  document.getElementById('mpmc').style.opacity = "1";
                }

                function mpmc_stop() {
                  document.getElementById('mpmc_image').style.opacity = "0";
                }
                mpmc_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2407.04036">
                <papertitle>Beyond Pixels: Semi-Supervised Semantic Segmentation with a Multi-scale Patch-based Multi-Label Classifier</papertitle>
              </a>
              <br>
              Prantik Howlader, <strong>Srijan Das</strong>, Hieu Le, Dimitris Samaras.
              <br>
							<em>

                                <strong>ECCV 2024</strong></em>
              <br>
              <a href="https://arxiv.org/abs/2407.04036">arXiv</a>
                /
              <a href="https://github.com/prantikbubun/Beyond-Pixels">code</a>
              <p></p>
              <p>
                A novel plug-in module designed for existing semi-supervised segmentation frameworks that offers patch-level supervision.
            </td>
          </tr>

          <tr onmouseout="bamm_stop()" onmouseover="bamm_start()" >
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/BAMM.gif' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function bamm_start() {
                  document.getElementById('bamm').style.opacity = "1";
                }

                function llavidal_stop() {
                  document.getElementById('bamm_image').style.opacity = "0";
                }
                bamm_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2406.09390">
                <papertitle>BAMM: Bidirectional Autoregressive Motion Model</papertitle>
              </a>
              <br>
              Ekkasit Pinyoanuntapong, Muhammad Usama Saleem, Pu Wang, Minwoo Lee, <strong>Srijan Das</strong>, Chen Chen.
              <br>
							<em>

                                <strong>ECCV 2024</strong></em>
              <br>
              <a href="https://arxiv.org/abs/2403.19435">arXiv</a>
                /
              <a href="https://exitudio.github.io/BAMM-page/">website</a>
              /
              <a href="https://github.com/exitudio/BAMM/">code</a>
              <p></p>
              <p>
                A novel text-to-motion generation framework. BAMM captures rich and bidirectional dependencies among motion tokens.
            </td>
          </tr>

          <tr onmouseout="pivit_stop()" onmouseover="pivit_start()" bgcolor="#ffffd0">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/pivit.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function pivit_start() {
                  document.getElementById('pivit').style.opacity = "1";
                }

                function pivit_stop() {
                  document.getElementById('pivit_image').style.opacity = "0";
                }
                pivit_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2311.18840">
                <papertitle>Just Add π! Pose Induced Video Transformers for Understanding Activities of Daily Living</papertitle>
              </a>
              <br>
              Dominick Reilly and <strong>Srijan Das</strong>.
              <br>


                         <em>       <strong>CVPR 2024</strong></em>
              <br>
              <a href="https://arxiv.org/abs/2311.18840">arXiv</a>
                /
              <a href="https://github.com/dominickrei/pi-vit">code</a>
              <p></p>
              <p>
						We introduce the first Pose Induced Video Transformer: PI-ViT (or π-ViT), a novel approach that augments the RGB representations learned by video transformers with 2D and 3D pose information.</p>
            </td>
          </tr>
           <tr onmouseout="simil_stop()" onmouseover="simil_start()" >
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/SI-MIL.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function pivit_start() {
                  document.getElementById('simil').style.opacity = "1";
                }

                function pivit_stop() {
                  document.getElementById('simil_image').style.opacity = "0";
                }
                simil_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2312.15010">
                <papertitle>SI-MIL: Taming Deep MIL for Self-Interpretability in Gigapixel Histopathology</papertitle>
              </a>
              <br>
              Saarthak Kapse<sup>*</sup>, Pushpak Pati<sup>*</sup>, <strong>Srijan Das</strong>, Jingwei Zhang, Chao Chen, Maria Vakalopoulou, Joel Saltz, Dimitris Samaras, Rajarsi Gupta, Prateek Prasanna.
              <br>
							<em> <strong>CVPR 2024</strong></em>
              <br>
              <a href="https://arxiv.org/abs/2312.15010">arXiv</a>
                /
              <a href="https://github.com/bmi-imaginelab/SI-MIL"> code </a>
              <p></p>
              <p>
					Self-Interpretable MIL (SI-MIL), the first interpretable-by-design MIL method for gigapixel WSIs, which provides de novo feature-level interpretations grounded on pathological insights for a WSI.</p>
            </td>
          </tr>
          <tr onmouseout="mavrec_stop()" onmouseover="mavrec_start()" >
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/mavrec.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function mavrec_start() {
                  document.getElementById('mavrec').style.opacity = "1";
                }

                function paat_stop() {
                  document.getElementById('mavrec_image').style.opacity = "0";
                }
                mavrec_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2312.04548">
                <papertitle>Multiview Aerial Visual Recognition (MAVREC): Can Multi-view Improve Aerial Visual Perception?</papertitle>
              </a>
              <br>
              Aritra Dutta, <strong> Srijan Das </strong>, Jacob Nielsen, Rajatsubhra Chakraborty, Mubarak Shah.
              <br>
						<em>	<strong>CVPR 2024</strong></em>
              <br>
              <a href="https://arxiv.org/abs/2312.04548">arXiv</a>
                /
              <a href="https://mavrec.github.io/">Website</a>
              <p></p>
              <p>
              We present MAVREC, a video dataset where we record synchronized scenes from different perspectives -- ground camera and drone-mounted camera. </p>
            </td>
          </tr>
          <tr onmouseout="dirl_stop()" onmouseover="dirl_start()">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/dirl.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function dirl_start() {
                  document.getElementById('DIRL').style.opacity = "1";
                }
                function dirl_stop() {
                  document.getElementById('DIRL').style.opacity = "0";
                }
                dirl_stop()
              </script>
           </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <a href="https://arxiv.org/abs/2309.06439">
                     <papertitle>Attention de-sparsification Matters: Inducing Diversity in Digital Pathology Representation Learning</papertitle>
               </a>
              <br>
              Saarthak Kapse, <strong>Srijan Das</strong>, Jingwei Zhang, Rajarsi R. Gupta, Joel Saltz, Dimitris Samaras, Prateek Prasanna.
              <br>
              <em> <strong> Medical Image Analysis (IF 10.9) </strong></em>
              <br>
              <a href="https://arxiv.org/abs/2309.06439">arXiv</a>
               /
              code (coming soon)
              <p> A diversity-inducing pretraining technique, tailored to enhance representation learning in digital pathology.</p>
            </td>
          </tr>
            <tr onmouseout="smalltr_stop()" onmouseover="smalltr_start()" bgcolor="#ffffd0">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/wacv24.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function smalltr_start() {
                  document.getElementById('smalltr').style.opacity = "1";
                }

                function paat_stop() {
                  document.getElementById('smalltr_image').style.opacity = "0";
                }
                smalltr_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2310.20704">
                <papertitle>Limited Data, Unlimited Potential: A Study on ViTs Augmented by Masked Autoencoders</papertitle>
              </a>
              <br>
              <strong>Srijan Das</strong>, Tanmay Jain, Dominick Reilly, Pranav Balaji, Soumyajit Karmakar, Shyam Marjit, Xiang Li, Abhijit Das, and Michael S. Ryoo.
              <br>
							<em><strong>WACV 2024</strong></em>
              <br>
              <a href="http://arxiv.org/abs/2310.20704">arXiv</a>
                /
              <a href="https://github.com/dominickrei/Limited-data-vits/">code</a>
              /
              <a href="images/wacv24_poster.pdf">Poster</a>
              /
              <a href="https://www.youtube.com/watch?v=yRBuBbUggA0&t=136s">Video</a>
              <p></p>
              <p>
						This paper shows that jointly optimizing ViTs for the primary task and a Self-Supervised Auxiliary Task is surprisingly beneficial when the amount of training data is limited.</p>
            </td>
          </tr>
          <tr>
        <td>
          <br><subheading class="pub-year">2023</subheading>
        </td>
        </tr>
            <tr onmouseout="clipad_stop()" onmouseover="clipad_start()">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/BMVC_AAN.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function clipad_start() {
                  document.getElementById('clipad').style.opacity = "1";
                }

                function paat_stop() {
                  document.getElementById('clipad_image').style.opacity = "0";
                }
                clipad_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2309.00696.pdf">
                <papertitle>Attributes-Aware Network for Temporal Action Detection</papertitle>
              </a>
              <br>
              Rui Dai, <strong>Srijan Das</strong>, Michael S. Ryoo, Francois Bremond.
              <br>
                <em><strong>BMVC 2023</strong></em>
              <br>
              <a href="https://arxiv.org/pdf/2309.00696.pdf"> arXiv </a> / <a href="https://youtu.be/PMj-UEZLXzg">video</a>
              <p></p>
              <p>
					This paper explains how to utilize OpenAI's CLIP for long-term action detection in videos.		</p>
            </td>
          </tr>
            <tr onmouseout="iccvwdf_stop()" onmouseover="iccvwdf_start()">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/ICCVW_DeepFake.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function iccvwdf_start() {
                  document.getElementById('iccvwdf').style.opacity = "1";
                }

                function iccvwdf_stop() {
                  document.getElementById('iccvwdf_image').style.opacity = "0";
                }
                iccvwdf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://arxiv.org/abs/2308.13503">
                <papertitle>Attending Generalizability in Course of Deep Fake Detection by Exploring Multi-task Learning</papertitle>
              </a>
              <br>
              Pranav Balaji, Abhijit Das, <strong>Srijan Das</strong>, Antitza Dantcheva.
              <br>
                <em><strong>Workshop and Challenge on DeepFake Analysis and Detection in ICCVW 2023</strong></em>
              <br>
              <a href="http://arxiv.org/abs/2308.13503">arXiv</a>
              <p></p>
              <p>
                  This paper investigates multi-task learning and contrastive techniques to evaluate their generalization benefits in DeepFake detection.</p>
            </td>
          </tr>

            <tr onmouseout="stc_stop()" onmouseover="stc_start()">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/stc-mix.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function stc20_start() {
                  document.getElementById('wstc_image').style.opacity = "1";
                }
                function stc_stop() {
                  document.getElementById('stc_image').style.opacity = "0";
                }
                stc_stop()
              </script>
           </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <a href="https://arxiv.org/abs/2112.03906">
                     <papertitle>Cross-modal Manifold Cutmix for Self-supervised Video Representation Learning</papertitle>
               </a>
              <br>
              <strong>Srijan Das</strong> and Michael S. Ryoo.
              <br>
							<em>18th International Conference on Machine Vision Applications </em>, July 2023
              <br>
                <a href="https://arxiv.org/abs/2112.03906">arXiv</a>
                /
                <a href="images/MVA23_poster.pdf"> Poster </a>
                /
                <a href="images/poster_award_mva.JPG" style="color: red; font-weight: bold;">Best Poster Award</a>
              <p> This paper focuses on designing video augmentation for self-supervised learning, we propose CMMC to make use of other modalities in videos for data mixing.</p>
            </td>
          </tr>

        <tr onmouseout="viewclr_stop()" onmouseover="viewclr_start()" bgcolor="#ffffd0">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/viewclr.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function viewclr_start() {
                  document.getElementById('viewclr_image').style.opacity = "1";
                }

                function rawnerf_stop() {
                  document.getElementById('viewclr_image').style.opacity = "0";
                }
                viewclr_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2112.03905">
                <papertitle>ViewCLR: Learning Self-supervised Video Representation for Unseen Viewpoints</papertitle>
              </a>
              <br>
              <strong>Srijan Das</strong>, and Michael S. Ryoo.
              <br>
							<em><strong>WACV 2023</strong></em>
              <br>
              <a href="https://arxiv.org/abs/2112.03905">arXiv</a>
              <p></p>
              <p>
								A framework for learning self-supervised video representation that is invariant to unseen camera viewpoints.</p>
            </td>
          </tr>

          <tr>
        <td>
          <br><subheading class="pub-year">2022</subheading>
        </td>
        </tr>

          <tr onmouseout="xiangssl_stop()" onmouseover="xiangssl_start()">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/Xiang_SSL.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function xiangssl_start() {
                  document.getElementById('Xiang_SSL').style.opacity = "1";
                }
                function xiangssl_stop() {
                  document.getElementById('Xing_SSL').style.opacity = "0";
                }
                xiangssl_stop()
              </script>
           </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <a href="https://arxiv.org/abs/2206.05266">
                     <papertitle>Does Self-supervised Learning Really Improve Reinforcement Learning from Pixels?</papertitle>
               </a>
              <br>
              Xiang Li, Jinghuan Shang, <strong>Srijan Das</strong>, Michael S. Ryoo.
              <br>
              <em><strong>NeurIPS 2022</strong></em>
              <br>
              <a href="https://arxiv.org/abs/2206.05266">arXiv</a>
                /
              <a href="https://github.com/LostXine/elo-sac">code</a>
              <p> The impacts of the existing self-supervised losses with  Joint Learning framework for RL is limited, while there is no golden method that can dominate all tasks. </p>
            </td>
          </tr>

          <tr onmouseout="trl_stop()" onmouseover="trl_start()">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/3dtrl.gif' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function trl_start() {
                  document.getElementById('TRL').style.opacity = "1";
                }
                function trl_stop() {
                  document.getElementById('TRL').style.opacity = "0";
                }
                trl_stop()
              </script>
           </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <a href="https://arxiv.org/pdf/2206.11895.pdf">
                     <papertitle>Learning Viewpoint-Agnostic Visual Representations by Recovering Tokens in 3D Space</papertitle>
               </a>
              <br>
              Jinghuan Shang, <strong>Srijan Das</strong>, Michael S. Ryoo.
              <br>
              <em><strong>NeurIPS 2022</strong></em>
              <br>
              <a href="https://arxiv.org/pdf/2206.11895.pdf">arXiv</a>
               /
              <a href="https://www3.cs.stonybrook.edu/~jishang/3dtrl/3dtrl.html">Project Page</a>
                /
              <a href="https://github.com/elicassion/3DTRL">code</a>
              <p> 3DTRL is a light-weighted, plug-and play layer that recovers 3D information of visual tokens and leverages it for learning viewpoint-agnostic representations.</p>
            </td>
          </tr>

          <tr onmouseout="tsu_stop()" onmouseover="tsu_start()" bgcolor="#ffffd0">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='tsu_image'><img src='imagesTSU/TSU_fig.png' height=175px width=250px></div>
                <img src='images/TSU_fig.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function tsu_start() {
                  document.getElementById('tsu_image').style.opacity = "1";
                }
                function tsu_stop() {
                  document.getElementById('tsu_image').style.opacity = "0";
                }
                tsu_stop()
              </script>
           </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <a href="https://arxiv.org/abs/2010.14982">
                     <papertitle>Toyota Smarthome Untrimmed: Real-World Untrimmed Videos for Activity Detection</papertitle>
               </a>
              <br>
              Rui Dai, <strong>Srijan Das</strong>, Saurav Sharma, Luca Minciullo, Lorenzo Garattoni, François Brémond, Gianpiero Francesca.
              <br>
              <em><strong>T-PAMI 2022</strong></em>
              <br>
              <br>
              <a href="https://project.inria.fr/toyotasmarthome/">Project Link</a> / <a href="https://github.com/dairui01/TSU_evaluation">Code</a>
              <p> TSU is a new untrimmed daily-living dataset consisting of 51 activities performed in a spontaneous manner, captured from non-optimal viewpoints.</p>
            </td>
          </tr>

        <tr onmouseout="mstct_stop()" onmouseover="mstct_start()">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/ms-tct.gif' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function mstct_start() {
                  document.getElementById('MSTCT').style.opacity = "1";
                }
                function mstct_stop() {
                  document.getElementById('MSTCT').style.opacity = "0";
                }
                mstct_stop()
              </script>
           </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <a href="https://arxiv.org/pdf/2112.03902.pdf">
                     <papertitle>MS-TCT: Multi-Scale Temporal ConvTransformer for Action Detection</papertitle>
               </a>
              <br>
              Rui Dai, <strong>Srijan Das</strong>, Kumara Kahatapitiya, Michael S. Ryoo, Francois Bremond.
              <br>
              <em><strong>CVPR 2022</strong></em>
              <br>
              <a href="https://arxiv.org/abs/2112.03902">arXiv</a>
               /
              <a href="https://github.com/dairui01/MS-TCT">code</a>
              <p> A ConvTransformer network that explores global and local temporal relations at multiple resolutions.</p>
            </td>
          </tr>
          <tr>
        <td>
          <br><subheading class="pub-year">2021</subheading>
        </td>
        </tr>

          <tr onmouseout="vpn++_stop()" onmouseover="vpn++_start()"  bgcolor="#ffffd0">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">

                <img src='images/vpn++.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function mip360_start() {
                  document.getElementById('vpn++_image').style.opacity = "1";
                }

                function mip360_stop() {
                  document.getElementById('vpn++_image').style.opacity = "0";
                }
                vpn++_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2105.08141">
                <papertitle>VPN++: Rethinking Video-Pose embeddings for understanding Activities of Daily Living</papertitle>
              </a>
              <br>
              <strong>Srijan Das</strong>, Rui Dai, Di Yang, Francois Bremond,
              <br>
							<em><strong>TPAMI</strong></em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2105.08141">arXiv</a>
              /
              <a href="https://github.com/srijandas07/vpnplusplus">code</a>
              <p></p>
              <p>VPN++ is an extension of our VPN model (ECCV 2020). VPN++ hallucinates pose driven features while not requiring costly 3D Poses at inference.</p>
            </td>
          </tr> 

          <tr onmouseout="bmvc21_stop()" onmouseover="bmvc21_start()">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/ctrn.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function bmvc21_start() {
                  document.getElementById('bmvc21_image').style.opacity = "1";
                }
                function bmvc21_stop() {
                  document.getElementById('bmvc21_image').style.opacity = "0";
                }
                tsu_stop()
              </script>
           </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <a href="https://www.bmvc2021-virtualconference.com/conference/papers/paper_0133.html">
                     <papertitle>CTRN: Class Temporal Relational Network for Action Detection</papertitle>
               </a>
              <br>
              Rui Dai, <strong>Srijan Das</strong>, Francois Bremond.
              <br>
              <em> <!--The British Machine Vision Conference, 2021. -->
                <strong>BMVC 2021, Oral</strong></em>
              <br>
            </td>
          </tr>

          <tr onmouseout="iccv21_stop()" onmouseover="iccv21_start()">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/dist_actdet.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function iccv21_start() {
                  document.getElementById('vpnp_image').style.opacity = "1";
                }
                function iccv21_stop() {
                  document.getElementById('vpnp_image').style.opacity = "0";
                }
                tsu_stop()
              </script>
           </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Dai_Learning_an_Augmented_RGB_Representation_With_Cross-Modal_Knowledge_Distillation_for_ICCV_2021_paper.html">
                     <papertitle>Learning an Augmented RGB Representation with Cross-Modal Knowledge Distillation for Action Detection</papertitle>
               </a>
              <br>
              Rui Dai, <strong>Srijan Das</strong>, Francois Bremond.
              <br>
              <em> <!--IEEE/CVF International Conference on Computer Vision, 2021. -->
                <strong>ICCV 2021</strong>
              <br>
            </td>
          </tr>

          <tr onmouseout="pdan_stop()" onmouseover="pdan_start()">
            <td style="padding:70px;width:25%;vertical-align:middle"
              <div class="one">
                <img src='images/pdan.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function pdan_start() {
                  document.getElementById('pdan_image').style.opacity = "1";
                }
                function pdan_stop() {
                  document.getElementById('pdan_image').style.opacity = "0";
                }
                vpn_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/WACV2021/papers/Dai_PDAN_Pyramid_Dilated_Attention_Network_for_Action_Detection_WACV_2021_paper.pdf">
                <papertitle>PDAN: Pyramid Dilated Attention Network for Action Detection.</papertitle>
              </a>
              <br>
              Rui Dai, <strong>Srijan Das</strong>, Luca Minciullo, Lorenzo Garattoni, Gianpiero Francesca and Francois Bremond.
              <br>
              <em><!--IEEE Winter Conference on Applications of Computer Vision</em>, 2021. -->
                <strong>WACV 2021</strong>
              <br>
              <a href="https://github.com/dairui01/PDAN">Code</a> / <a href="https://www.youtube.com/watch?v=lvEcjufIkdo">Video</a> / <a href="research/RLDL_poster.pdf">Poster</a>
            </td>
          </tr>
          <tr>
        <td>
          <br><subheading class="pub-year">2020</subheading>
        </td>
        </tr>
  <tr onmouseout="vpn_stop()" onmouseover="vpn_start()" bgcolor="#ffffd0">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/vpn.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function vpn_start() {
                  document.getElementById('vpn_image').style.opacity = "1";
                }
                function vpn_stop() {
                  document.getElementById('vpn_image').style.opacity = "0";
                }
                vpn_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2007.03056">
                <papertitle>VPN: Learning Video-Pose Embedding for Activities of Daily Living</papertitle>
              </a>
              <br>
              <strong>Srijan Das</strong>, Saurav Sharma, Rui Dai, Francois Bremond, Monique Thonnat.
              <br>
              <em><!--16th European Conference on Computer Vision</em>, 2020.-->
                <strong>ECCV 2020</strong>
              <br>
              <br>
              <a href="https://github.com/srijandas07/VPN">Code</a>
            </td>
          </tr>

          <tr onmouseout="wacv20_stop()" onmouseover="wacv20_start()">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/temporal_model.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function wacv20_start() {
                  document.getElementById('wacv20_image').style.opacity = "1";
                }
                function wacv20_stop() {
                  document.getElementById('wacv20_image').style.opacity = "0";
                }
                tsu_stop()
              </script>
           </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <a href="https://openaccess.thecvf.com/content_WACV_2020/html/Das_Looking_deeper_into_Time_for_Activities_of_Daily_Living_Recognition_WACV_2020_paper.html">
                     <papertitle>Looking deeper into Time for Activities of Daily Living Recognition</papertitle>
               </a>
              <br>
              <strong>Srijan Das</strong>, Monique Tonnat and Francois Bremond.
              <br>
              <em> <!--Winter Conference on Applications of Computer Vision, 2020. -->
                <strong>WACV 2020</strong></em>
              <br>
            </td>
          </tr>
        <tr>
        <td>
          <br><subheading class="pub-year">2019</subheading>
        </td>
        </tr>
        <tr onmouseout="sta_stop()" onmouseover="sta_start()" bgcolor="#ffffd0">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/STA_fig.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function stego_start() {
                  document.getElementById('sta_image').style.opacity = "1";
                }

                function stego_stop() {
                  document.getElementById('sta_image').style.opacity = "0";
                }
                stego_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Das_Toyota_Smarthome_Real-World_Activities_of_Daily_Living_ICCV_2019_paper.pdf">
                <papertitle>Toyota Smarthome: Real World Activities of Daily Living.</papertitle>
              </a>
              <br>
              <strong>Srijan Das</strong>, Rui Dai, Michal Koperski, Luca Minciullo, Lorenzo Garattoni, Francois Bremond and Gianpiero Francesca.
              <br>
              <em><!--In Proceedings of the 17th International Conference on Computer Vision</em>, 2019.-->
                <strong>ICCV 2019</strong>
              <br>
              <br>
              <a href="https://project.inria.fr/toyotasmarthome/">Project Link</a> / <a href="https://github.com/DAIGroup/separable_STA">Code</a>
            </td>
          </tr>
          <tr onmouseout="wacv19_stop()" onmouseover="wacv19_start()">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/P-i3d.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function wacv19_start() {
                  document.getElementById('wacv19_image').style.opacity = "1";
                }
                function wacv19_stop() {
                  document.getElementById('wacv19_image').style.opacity = "0";
                }
                tsu_stop()
              </script>
           </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <a href="https://ieeexplore.ieee.org/document/8658564">
                     <papertitle>Where to focus on for Human Action Recognition?</papertitle>
               </a>
              <br>
              <strong>Srijan Das</strong>, Arpit Chaudhary, Francois Bremond and Monique Thonnat.
              <br>
              <em> <!--Winter Conference on Applications of Computer Vision, 2019. -->
                <strong>WACV 2019</strong></em>
              <br>
            </td>
          </tr>
        </p>



          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Datasets</heading>
              <p style="margin-left: 40px">
              <ul>
                <li><b><em> <a href="https://project.inria.fr/toyotasmarthome/">Toyota Smarthome Trimmed (2019)</a> </em></b>   </li>
                <li><b><em><a href="https://project.inria.fr/toyotasmarthome/">Toyota Smarthome Untrimmed (2021)</a></em></b>  </li>
                <li><b><em><a href="https://mavrec.github.io/">MAVREC Dataset (2023)</a></em></b> &nbsp;&nbsp; </li>
              <li><b><em><a href="https://adl-x.github.io/">ADL-X Dataset (2024)</a></em></b> &nbsp;&nbsp; </li>
              </ul>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Talks</heading>
              <p style="margin-left: 40px">
              <ul>
                <li><b><em>Mar 2024</em></b> &nbsp;&nbsp; Talk on "Computer Vision Projects in CharMLab" in a RoundTable discussion on AI in conjunction with the Defense Alliance of NC (DANC) and the Michael Best Law Firm.
                <li><b><em>Feb 2024</em></b> &nbsp;&nbsp; Invited Online Tech Talk on "From Pixels to Robots: Recipes for Vision-Enabled Robot Learning" at Christ University, Bangalore, India.
                <li><b><em>Dec 2023</em></b> &nbsp;&nbsp; Invited Talk on "Video Understanding using AI" as part of the "AI and ROS for Robotics: Theory and Practice" short-term training program at IIITDM.
                <li><b><em>Jun 2023</em></b> &nbsp;&nbsp; Invited Talk on "Computer Vision for Robot Learning" as part of the "AI and Machine Vision for Robotics" short-term training program at IIITDM. (Virtually)  </li>
                <li><b><em>Apr 2023</em></b> &nbsp;&nbsp; Talk on "From Few to More: Enhancing ViT Performance on Limited Data" at PHPC Lab in UNC Charlotte.  </li>
                <li><b><em>Mar 2023</em></b> &nbsp;&nbsp; Talk on "From Pixels to Robots: Recipes for Vision-Enabled Robot Learning" at the Seminar on Controls and Robotics in UNC Charlotte.  </li>
                <li><b><em>Jan 2023</em></b> &nbsp;&nbsp; Talk on "Quo vadis, computer vision!" at the PhD seminar in UNC Charlotte.  </li>
                <li><b><em>Mar 2022</em></b> &nbsp;&nbsp; Invited Talk in AICTE sponsored Short Term Course on "Multiple Modalities are all you need for Video Understanding!" at IIITDM Kancheepuram. (Virtually)  </li>
                <li><b><em>Sep 2021</em></b> &nbsp;&nbsp; Talk on "Vision for understanding Activities of Daily Living" at <a href="https://www.linkedin.com/company/scitech-talks/" color="blue">SciTech Talks </a>. <a href="https://www.youtube.com/watch?v=YNFUMQkWBSk" color="blue">[video] </a> </li>
                <li><b><em>Apr 2021</em></b> &nbsp;&nbsp; Seminar talk on "How to combine modalities for understanding Activities of Daily Living? " for CSE 600 at Stony Brook University, NY, USA. </li>
                <li><b><em>Nov 2020</em></b> &nbsp;&nbsp; Seminar talk on "How to combine RGB & Poses for understanding Activities of Daily Living?" at Université Lumière Lyon 2. </li>
                <li><b><em>Nov 2019</em></b> &nbsp;&nbsp; <a href="https://www.meetup.com/fr-FR/Data-Science-Meetup-Nice-Sophia-Antipolis/events/266551193/" color="blue">Nice Data Science meetup </a>. <a href="https://drive.google.com/file/d/1mJe3Vh75wdZojeS3dxLPNOYWUIyi0c5O/view" color="blue">[slides] </a></li>
                <li><b><em>Aug 2018</em></b> &nbsp;&nbsp; <a href="http://www.innovation-alzheimer.fr/summer-school-program-2018/" color="blue">Summer School Brain Innovation Generation @ UCA </a>.  <a href="https://www.dropbox.com/s/ci4m4o2cqkpt85e/UCA_summer_school.pdf?dl=0" color="blue">[slides] </a></li>
              </ul>
              </p>
            </td>
          </tr>
        </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Teaching</heading>
                <p style="margin-left: 40px">
              <ul>
              <li><b><em>Fall 2024</em></b> <a href="data/6010_8010_Syllabus.pdf">ITCS 6010/8010 Advanced Computer Vision (Topics Course)</a> </li>
                <li><b><em>Fall 2022, Spring 2023, Fall 2023, Spring 2024</em></b> <a href="data/4152_5152_Syllabus.pdf">ITCS 4152/5152 Computer Vision</a> </li>

                <li><b><em>Aug 2021</em></b> &nbsp;&nbsp; Surviving the Deep Learning Apocalypse (SKFGI Webinar series 2020) </li>
                <p style="margin-left: 80px">
              <ol type="1">
              <li>  Video Understanding: How to model Time? <a href="https://www.youtube.com/watch?v=bNINKn6S3Gs" color="blue"> [video]</a> </li>
              <li>  Tips to attend attention! <a href="https://www.youtube.com/watch?v=iBx9EvCdAC4" color="blue"> [video]</a> </li>
            </ol>
              <p>
                <li><b><em>Jan 2021</em></b> &nbsp;&nbsp; <a href="http://www-sop.inria.fr/members/Francois.Bremond/MSclass/deepLearningWinterSchool/UCA_master/index.html" color="blue">Deep Learning Winter School for Computer Vision 2019-20 </li> </p>
                <p style="margin-left: 80px">
              <ol type="1">
              <li>  Introduction to video classification & RNN. <a href="http://www-sop.inria.fr/members/Francois.Bremond/MSclass/deepLearningWinterSchool/slides/Lecture_Srijan.pdf" color="blue">[slides] </a> <a href="https://drive.google.com/file/d/1c4z9lAXdkqf1Ak7SHPtAWYwpg0hiVCQ8/view" color="blue">[assignment1] </a> <a href="https://colab.research.google.com/drive/1vZ8jaj5pMlHNnjF4KkBWf_7-lDpvxBeW" color="blue">[Practical 1]</a>  </li>
              <li>  Action Classification in videos <a href="http://www-sop.inria.fr/members/Francois.Bremond/MSclass/deepLearningWinterSchool/slides/Lecture_2.pdf" color="blue">[slides] </a> </li>
              <li>  Attention Mechanisms for video analytics <a href="http://www-sop.inria.fr/members/Francois.Bremond/MSclass/deepLearningWinterSchool/slides/Lecture_3.pdf" color="blue">[slides] </a> <a href="https://www.dropbox.com/s/miptig2m0qkbytg/Assignment_teaching.pdf?dl=0" color="blue">[assignment2] </a> <a href="http://www-sop.inria.fr/members/Francois.Bremond/MSclass/deepLearningWinterSchool/slides/Practical.pdf" color="blue">[Practical 2]</a> </li>
                </ol>
            </ul>
              </p>
            </td>
          </tr>
        </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Academic Activities</heading>
              <p style="margin-left: 40px">
              <ul>
                <li>Program committee member of AAAI-24 Student Program.</li>
                <li>Associate Editor for ICRA 2024.</li>
                <li>Member of DEI committe for CVPR 2023.</li>
                <li>Senior Program Committee Member for AAAI 2023 and AAAI 2024.</li>
                <li>Session chair for Image Understanding & Activity Recognition session at IPAS 2020. </li>
                <li>Mentored for B.E.N.J.I. in GirlScript Summer of Code 2019 edition. </li>
                <li>Mentor for the Emerging Technology Business Incubator (ETBI) Led by NIT Rourkela, a platform envisaged to transform the start-up ecosystem of the region. </li>
                <li>Reviewer at ICACIE 2017, 2018, SETIT 2018, KCST 2019, ICAML 2019, AVSS 2019, 2022, WACV 2020, 2021, 2022, CVPR 2021, 2022, 2023, 2024, ECCV 2022, 2024, ICCV 2021, 2023, AAAI 2023, NeurIPS 2023, IROS 2021, 2024.</li>
                <li>Reviewer at TPAMI, Pattern Recognition, Elsevier Journal of CVIU, Elsevier Journal of FGCS, Elsevier Journal of Computer & Electrical Engineering, MTAP, and Journal of Signal Processing: Image Communication. </li>
                <li>Volunteer at ICACNI 2014, ICACNI 2016, ICCV 2019, ICLR 2020 & ICML 2020. </li>
                </ul>
              </p>
            </td>
          </tr>
        </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Thanks to Jon Barron for the theme.</a>
                <br>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>

</body>

</html>
