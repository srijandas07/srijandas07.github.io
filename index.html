<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Srijan Das</title>
  
  <meta name="author" content="Srijan Das">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Srijan Das</name>
              </p>
              <p>I am an Assistant Professor in the Department of Computer Science at the University of North Carolina at Charlotte. At UNC Charlotte, I am working on Video Representation Learning, and Robotic Vision. I am a member of the Charlotte Machine Learning Lab (<a href="https://charlotteml.github.io/">CharMLab</a>) at UNC Charlotte.
              </p>
              <p>
                  Before this, I was a Postdoctoral Associate at Stony Brook University under the supervision of <a href="http://michaelryoo.com/">Michael Ryoo</a>. In 2020, I completed my Ph.D. in Computer Science at INRIA, Sophia Antipolis, France under the supervision of <a href="http://www-sop.inria.fr/members/Francois.Bremond/">Francois Bremond</a> and <a href="http://www-sop.inria.fr/members/Monique.Thonnat/">Monique Thonnat</a>.
                My Ph.D. thesis is on <a href="https://hal.archives-ouvertes.fr/tel-02973812/document">¨Spatio-temporal attention mechanisms for Action Recognition¨</a> and click <a href="https://www.youtube.com/watch?v=HIsqMt9dA78">here</a> to watch my Defense Presentation.
                I did my Post-Grad in Computer Science from the <a href="https://www.nitrkl.ac.in/">National Institute of Technology (NIT), Rourkela</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:sdas24@uncc.edu">Email</a> &nbsp/&nbsp
                <a href="data/srijan_CV.pdf">CV</a> &nbsp/&nbsp
                <a href="data/srijan-bio.txt">Bio</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=ZDTF5AEAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/srijandas07">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/srijandas07/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/srijan_circle_new.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/srijan_circle_new.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I am interested in computer vision, machine learning, deep learning, and image processing. I have been mostly working on video representation learning including spatio-temporal attention mechanisms, cross-modal attention mechanisms, cross-modal knowledge distillation, and self-supervised learning for applications like action classification in trimmed videos, temporal action detection in untrimmed videos, video retrieval, anomaly detection, and deepfake detection.
              </p>
              <p>
                [<strong>Hiring</strong>] I am actively looking for motivated graduate students who can conduct research with me in UNC Charlotte. If your research interest aligns with mine, please apply to the UNC Charlotte graduate <a href="https://gradadmissions.charlotte.edu/apply"> application </a> for a PhD and mention my name in your application. Please feel free to also reach out to me if you are really interested in working on this area!
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p style="margin-left: 40px">
              <ul>
                <li><b>Oct 2023</b> - One paper accepted to WACV 2024. </li>
                <li><b>Aug 2023</b> - One paper on DeepFake detection has been accepted at ICCVW 2023, and another paper on using CLIP for Action Detection has been accepted at BMVC 2023.</li>
                <li><b>Jul 2023</b> - CMMC received the <a href="images/poster_award_mva.JPG"> Best Poster Award </a>a at MVA 2023.</li>
                <li><b>May 2023</b> - Serving as SPC for AAAI 2024 respectively.</li>
                <li><b>May 2023</b> - Dominick has been selected as a recipient of the Chateaubriand Fellowship. Congratulations!</li>
                <li><b>Apr 2023</b> - Serving as a member of the DEI committee for CVPR 2023.</li>
                <li><b>Feb 2023</b> - First NSF Grant has been awarded. [<a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2245652&HistoricalAwards=false">Link</a>]</li>
                <li><b>Jan 2023</b> - One paper with colleagues at Stony Brook Medicine is accepted to ISBI 2023.</li>
                <li><b>Sep 2022</b> - Two papers accepted to NeurIPS 2022.</li>
                <li><b>Aug 2022</b> - One paper accepted to WACV 2023 (first round).</li>
                <li><b>Aug 2022</b> - Joined UNC Charlotte as an Asst. Professor.</li>
                <li><b>Jul 2022</b> - We are organizing a workshop "Artificial Intelligence for Automated Human Health-care and Monitoring" at IEEE FG 2023.</li>
                <li><b>Jul 2022</b> - Serving as Senior Program Committee Member for AAAI 2023.</li>
                <li><b>Jun 2022</b> - Secured 2nd-place in <a href="http://ego4d-data.org/Workshop/CVPR22/">Ego4D challenge</a> under the Long-Term Anticipation Track at CVPR 2022. [<a href="https://arxiv.org/abs/2207.00579">Report</a>][<a href="https://github.com/srijandas07/clip_baseline_LTA_Ego4d">Code</a>]</li>
                <li><b>Apr 2022</b> - Toyota Smarthome Untrimmed (TSU) has been accepted to TPAMI.</li>
                <li><b>Mar 2022</b> - One Paper accepted to CVPR 2022.</li>
                <li><b>Dec 2021</b> - We have organized a special session on <a href='http://iab-rubric.org/fg2021/session.html'>Applications in Healthcare and Health Monitoring</a> at the 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG'21). </li>
                <li><b>Dec 2021</b> - Two papers on DeepFake detection and Video Anomaly detection accepted at FG 2021. </li>
                <li><b>Nov 2021</b> - One paper on Video Anomaly Detection is presented in AVSS 2021.</li>
                <li><b>Nov 2021</b> - Papers accepted in ICCV 2021, BMVC 2021 (Oral), TPAMI.</li>
                <li><b>Apr 2021</b> - Joined Stony Brook University as a Postdoctoral Associate.</li>
                <li><b>Nov 2020</b> - Session chair for Image Understanding & Activity Recognition session at <a href='https://ipas.ieee.tn/'><strong>IPAS 2020</strong></a>.</li>

              </ul>
              </p>
            </td>
          </tr>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Lab members</heading>
                <style>
              .lab-member {
                font-size: 1em; /* change the font size to your desired size */
                text-decoration: underline; /* add an underline */
                }
            </style>
              <p style="margin-left: 40px">
              <ul>
            <subheading class="lab-member">Current Students</subheading>

                <li><em><a href="https://dominickrei.github.io/">Dominick Reilly</a> (PhD student at UNC Charlotte) </em>
              <li><em><a href="https://webpages.charlotte.edu/asinha13/">Arkaprava Sinha</a> (PhD student at UNC Charlotte) </em>
              <li><em> <a href="https://webpages.charlotte.edu/rchakra6/">Rajatsubhra Chakraborty</a> (PhD student at UNC Charlotte) </em>
                <li><em>Vishal Bondili (Master student at UNC Charlotte) </em>

              </ul>
               </p>
                <p style="margin-left: 40px">
              <ul>
                <subheading class="lab-member">Past Students</subheading>
                    <li><em>Jonathan Lorray (Master student at UNC Charlotte) </em>
                    <li><em>Jacob Nielsen (jointly supervised with Prof. Aritra Dutta at SDU)</em>
                    <li><em>Ian Boyle (undergraduate at UNC Charlotte), 2022-23 </em>
                    <li><em>Tanmay Jain (intern from DTU, India), 2022-23 </em>
                    <li><em>Soumyajit Karmakar (intern from IIIT Guwahati, India), 2022-23 </em>
                    <li><em>Shyam Marjit (intern from IIIT Guwahati, India), 2022-23 </em>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><p>
        <heading>Selected Publications</heading> (For full list of papers, visit my <a href="https://scholar.google.com/citations?user=ZDTF5AEAAAAJ">Google Scholar</a>.) <br>
          <tr>
          <td>
            <style>
              .pub-year {
                font-size: 2em; /* change the font size to your desired size */
                text-decoration: underline; /* add an underline */
                }
            </style>
            <subheading class="pub-year">Preprint & 2024</subheading>
          </td>
          </tr>
        <p>
            <tr onmouseout="smalltr_stop()" onmouseover="smalltr_start()" bgcolor="#ffffd0">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/wacv24.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function smalltr_start() {
                  document.getElementById('smalltr').style.opacity = "1";
                }

                function paat_stop() {
                  document.getElementById('smalltr_image').style.opacity = "0";
                }
                smalltr_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2310.20704">
                <papertitle>Limited Data, Unlimited Potential: A Study on ViTs Augmented by Masked Autoencoders</papertitle>
              </a>
              <br>
              <strong>Srijan Das</strong>, Tanmay Jain, Dominick Reilly, Pranav Balaji, Soumyajit Karmakar, Shyam Marjit, Xiang Li, Abhijit Das, and Michael S. Ryoo.
              <br>
							<em><strong>WACV2024</strong></em>
              <br>
              <a href="http://arxiv.org/abs/2310.20704">arXiv</a>
                /
              <a href="https://github.com/dominickrei/Limited-data-vits/">code</a>
              <p></p>
              <p>
						This paper shows that jointly optimizing ViTs for the primary task and a Self-Supervised Auxiliary Task is surprisingly beneficial when the amount of training data is limited.</p>
            </td>
          </tr>
          <tr>
        <td>
          <br><subheading class="pub-year">2023</subheading>
        </td>
        </tr>
            <tr onmouseout="clipad_stop()" onmouseover="clipad_start()">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/BMVC_AAN.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function clipad_start() {
                  document.getElementById('clipad').style.opacity = "1";
                }

                function paat_stop() {
                  document.getElementById('clipad_image').style.opacity = "0";
                }
                clipad_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2309.00696.pdf">
                <papertitle>Attributes-Aware Network for Temporal Action Detection</papertitle>
              </a>
              <br>
              Rui Dai, <strong>Srijan Das</strong>, Michael S. Ryoo, Francois Bremond.
              <br>
                <em><strong>BMVC 2023</strong></em>
              <br>
              <a href="https://arxiv.org/pdf/2309.00696.pdf"> arXiv </a> / <a href="https://youtu.be/PMj-UEZLXzg">video</a>
              <p></p>
              <p>
					This paper explains how to utilize OpenAI's CLIP for long-term action detection in videos.		</p>
            </td>
          </tr>
            <tr onmouseout="iccvwdf_stop()" onmouseover="iccvwdf_start()">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/ICCVW_DeepFake.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function iccvwdf_start() {
                  document.getElementById('iccvwdf').style.opacity = "1";
                }

                function iccvwdf_stop() {
                  document.getElementById('iccvwdf_image').style.opacity = "0";
                }
                iccvwdf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://arxiv.org/abs/2308.13503">
                <papertitle>Attending Generalizability in Course of Deep Fake Detection by Exploring Multi-task Learning</papertitle>
              </a>
              <br>
              Pranav Balaji, Abhijit Das, <strong>Srijan Das</strong>, Antitza Dantcheva.
              <br>
                <em><strong>Workshop and Challenge on DeepFake Analysis and Detection in ICCVW 2023</strong></em>
              <br>
              <a href="http://arxiv.org/abs/2308.13503">arXiv</a>
              <p></p>
              <p>
                  This paper investigates multi-task learning and contrastive techniques to evaluate their generalization benefits in DeepFake detection.</p>
            </td>
          </tr>
            <tr onmouseout="dirl_stop()" onmouseover="dirl_start()">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/dirl.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function dirl_start() {
                  document.getElementById('DIRL').style.opacity = "1";
                }
                function dirl_stop() {
                  document.getElementById('DIRL').style.opacity = "0";
                }
                dirl_stop()
              </script>
           </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <a href="https://openreview.net/pdf?id=SUcUqu_X30">
                     <papertitle>Attention de-sparsification Matters: Inducing Diversity in Digital Pathology Representation Learning</papertitle>
               </a>
              <br>
              Saarthak Kapse, <strong>Srijan Das</strong>, Jingwei Zhang, Rajarsi R. Gupta, Joel Saltz, Dimitris Samaras, Prateek Prasanna.
              <br>
                <em>Preprint</em>
              <br>
              <a href="https://openreview.net/pdf?id=SUcUqu_X30">OpenReview</a>
               /
              code (coming soon)
              <p> A diversity-inducing pretraining technique, tailored to enhance representation learning in digital pathology.</p>
            </td>
          </tr>

            <tr onmouseout="stc_stop()" onmouseover="stc_start()">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/stc-mix.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function stc20_start() {
                  document.getElementById('wstc_image').style.opacity = "1";
                }
                function stc_stop() {
                  document.getElementById('stc_image').style.opacity = "0";
                }
                stc_stop()
              </script>
           </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <a href="https://arxiv.org/abs/2112.03906">
                     <papertitle>Cross-modal Manifold Cutmix for Self-supervised Video Representation Learning</papertitle>
               </a>
              <br>
              <strong>Srijan Das</strong> and Michael S. Ryoo.
              <br>
							<em>18th International Conference on Machine Vision Applications </em>, July 2023
              <br>
                <a href="https://arxiv.org/abs/2112.03906">arXiv</a>
                /
                <a href="images/MVA23_poster.pdf"> Poster </a>
                /
                <a href="images/poster_award_mva.JPG" style="color: red; font-weight: bold;">Best Poster Award</a>
              <p> This paper focuses on designing video augmentation for self-supervised learning, we propose CMMC to make use of other modalities in videos for data mixing.</p>
            </td>
          </tr>

        <tr onmouseout="viewclr_stop()" onmouseover="viewclr_start()" bgcolor="#ffffd0">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/viewclr.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function viewclr_start() {
                  document.getElementById('viewclr_image').style.opacity = "1";
                }

                function rawnerf_stop() {
                  document.getElementById('viewclr_image').style.opacity = "0";
                }
                viewclr_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2112.03905">
                <papertitle>ViewCLR: Learning Self-supervised Video Representation for Unseen Viewpoints</papertitle>
              </a>
              <br>
              <strong>Srijan Das</strong>, and Michael S. Ryoo.
              <br>
							<em><strong>WACV 2023</strong></em>
              <br>
              <a href="https://arxiv.org/abs/2112.03905">arXiv</a>
              <p></p>
              <p>
								A framework for learning self-supervised video representation that is invariant to unseen camera viewpoints.</p>
            </td>
          </tr>

          <tr>
        <td>
          <br><subheading class="pub-year">2022</subheading>
        </td>
        </tr>

          <tr onmouseout="xiangssl_stop()" onmouseover="xiangssl_start()">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/Xiang_SSL.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function xiangssl_start() {
                  document.getElementById('Xiang_SSL').style.opacity = "1";
                }
                function xiangssl_stop() {
                  document.getElementById('Xing_SSL').style.opacity = "0";
                }
                xiangssl_stop()
              </script>
           </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <a href="https://arxiv.org/abs/2206.05266">
                     <papertitle>Does Self-supervised Learning Really Improve Reinforcement Learning from Pixels?</papertitle>
               </a>
              <br>
              Xiang Li, Jinghuan Shang, <strong>Srijan Das</strong>, Michael S. Ryoo.
              <br>
              <em><strong>NeurIPS 2022</strong></em>
              <br>
              <a href="https://arxiv.org/abs/2206.05266">arXiv</a>
                /
              <a href="https://github.com/LostXine/elo-sac">code</a>
              <p> The impacts of the existing self-supervised losses with  Joint Learning framework for RL is limited, while there is no golden method that can dominate all tasks. </p>
            </td>
          </tr>

          <tr onmouseout="trl_stop()" onmouseover="trl_start()">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/3dtrl.gif' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function trl_start() {
                  document.getElementById('TRL').style.opacity = "1";
                }
                function trl_stop() {
                  document.getElementById('TRL').style.opacity = "0";
                }
                trl_stop()
              </script>
           </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <a href="https://arxiv.org/pdf/2206.11895.pdf">
                     <papertitle>Learning Viewpoint-Agnostic Visual Representations by Recovering Tokens in 3D Space</papertitle>
               </a>
              <br>
              Jinghuan Shang, <strong>Srijan Das</strong>, Michael S. Ryoo.
              <br>
              <em><strong>NeurIPS 2022</strong></em>
              <br>
              <a href="https://arxiv.org/pdf/2206.11895.pdf">arXiv</a>
               /
              <a href="https://www3.cs.stonybrook.edu/~jishang/3dtrl/3dtrl.html">Project Page</a>
                /
              <a href="https://github.com/elicassion/3DTRL">code</a>
              <p> 3DTRL is a light-weighted, plug-and play layer that recovers 3D information of visual tokens and leverages it for learning viewpoint-agnostic representations.</p>
            </td>
          </tr>

          <tr onmouseout="tsu_stop()" onmouseover="tsu_start()" bgcolor="#ffffd0">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='tsu_image'><img src='imagesTSU/TSU_fig.png' height=175px width=250px></div>
                <img src='images/TSU_fig.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function tsu_start() {
                  document.getElementById('tsu_image').style.opacity = "1";
                }
                function tsu_stop() {
                  document.getElementById('tsu_image').style.opacity = "0";
                }
                tsu_stop()
              </script>
           </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <a href="https://arxiv.org/abs/2010.14982">
                     <papertitle>Toyota Smarthome Untrimmed: Real-World Untrimmed Videos for Activity Detection</papertitle>
               </a>
              <br>
              Rui Dai, <strong>Srijan Das</strong>, Saurav Sharma, Luca Minciullo, Lorenzo Garattoni, François Brémond, Gianpiero Francesca.
              <br>
              <em><strong>T-PAMI 2022</strong></em>
              <br>
              <br>
              <a href="https://project.inria.fr/toyotasmarthome/">Project Link</a> / <a href="https://github.com/dairui01/TSU_evaluation">Code</a>
              <p> TSU is a new untrimmed daily-living dataset consisting of 51 activities performed in a spontaneous manner, captured from non-optimal viewpoints.</p>
            </td>
          </tr>

        <tr onmouseout="mstct_stop()" onmouseover="mstct_start()">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/ms-tct.gif' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function mstct_start() {
                  document.getElementById('MSTCT').style.opacity = "1";
                }
                function mstct_stop() {
                  document.getElementById('MSTCT').style.opacity = "0";
                }
                mstct_stop()
              </script>
           </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <a href="https://arxiv.org/pdf/2112.03902.pdf">
                     <papertitle>MS-TCT: Multi-Scale Temporal ConvTransformer for Action Detection</papertitle>
               </a>
              <br>
              Rui Dai, <strong>Srijan Das</strong>, Kumara Kahatapitiya, Michael S. Ryoo, Francois Bremond.
              <br>
              <em><strong>CVPR 2022</strong></em>
              <br>
              <a href="https://arxiv.org/abs/2112.03902">arXiv</a>
               /
              <a href="https://github.com/dairui01/MS-TCT">code</a>
              <p> A ConvTransformer network that explores global and local temporal relations at multiple resolutions.</p>
            </td>
          </tr>
          <tr>
        <td>
          <br><subheading class="pub-year">2021</subheading>
        </td>
        </tr>

          <tr onmouseout="vpn++_stop()" onmouseover="vpn++_start()"  bgcolor="#ffffd0">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">

                <img src='images/vpn++.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function mip360_start() {
                  document.getElementById('vpn++_image').style.opacity = "1";
                }

                function mip360_stop() {
                  document.getElementById('vpn++_image').style.opacity = "0";
                }
                vpn++_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2105.08141">
                <papertitle>VPN++: Rethinking Video-Pose embeddings for understanding Activities of Daily Living</papertitle>
              </a>
              <br>
              <strong>Srijan Das</strong>, Rui Dai, Di Yang, Francois Bremond,
              <br>
							<em><strong>TPAMI</strong></em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2105.08141">arXiv</a>
              /
              <a href="https://github.com/srijandas07/vpnplusplus">code</a>
              <p></p>
              <p>VPN++ is an extension of our VPN model (ECCV 2020). VPN++ hallucinates pose driven features while not requiring costly 3D Poses at inference.</p>
            </td>
          </tr> 

          <tr onmouseout="bmvc21_stop()" onmouseover="bmvc21_start()">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/ctrn.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function bmvc21_start() {
                  document.getElementById('bmvc21_image').style.opacity = "1";
                }
                function bmvc21_stop() {
                  document.getElementById('bmvc21_image').style.opacity = "0";
                }
                tsu_stop()
              </script>
           </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <a href="https://www.bmvc2021-virtualconference.com/conference/papers/paper_0133.html">
                     <papertitle>CTRN: Class Temporal Relational Network for Action Detection</papertitle>
               </a>
              <br>
              Rui Dai, <strong>Srijan Das</strong>, Francois Bremond.
              <br>
              <em> <!--The British Machine Vision Conference, 2021. -->
                <strong>BMVC 2021, Oral</strong></em>
              <br>
            </td>
          </tr>

          <tr onmouseout="iccv21_stop()" onmouseover="iccv21_start()">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/dist_actdet.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function iccv21_start() {
                  document.getElementById('vpnp_image').style.opacity = "1";
                }
                function iccv21_stop() {
                  document.getElementById('vpnp_image').style.opacity = "0";
                }
                tsu_stop()
              </script>
           </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Dai_Learning_an_Augmented_RGB_Representation_With_Cross-Modal_Knowledge_Distillation_for_ICCV_2021_paper.html">
                     <papertitle>Learning an Augmented RGB Representation with Cross-Modal Knowledge Distillation for Action Detection</papertitle>
               </a>
              <br>
              Rui Dai, <strong>Srijan Das</strong>, Francois Bremond.
              <br>
              <em> <!--IEEE/CVF International Conference on Computer Vision, 2021. -->
                <strong>ICCV 2021</strong>
              <br>
            </td>
          </tr>

          <tr onmouseout="pdan_stop()" onmouseover="pdan_start()">
            <td style="padding:70px;width:25%;vertical-align:middle"
              <div class="one">
                <img src='images/pdan.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function pdan_start() {
                  document.getElementById('pdan_image').style.opacity = "1";
                }
                function pdan_stop() {
                  document.getElementById('pdan_image').style.opacity = "0";
                }
                vpn_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/WACV2021/papers/Dai_PDAN_Pyramid_Dilated_Attention_Network_for_Action_Detection_WACV_2021_paper.pdf">
                <papertitle>PDAN: Pyramid Dilated Attention Network for Action Detection.</papertitle>
              </a>
              <br>
              Rui Dai, <strong>Srijan Das</strong>, Luca Minciullo, Lorenzo Garattoni, Gianpiero Francesca and Francois Bremond.
              <br>
              <em><!--IEEE Winter Conference on Applications of Computer Vision</em>, 2021. -->
                <strong>WACV 2021</strong>
              <br>
              <a href="https://github.com/dairui01/PDAN">Code</a> / <a href="https://www.youtube.com/watch?v=lvEcjufIkdo">Video</a> / <a href="research/RLDL_poster.pdf">Poster</a>
            </td>
          </tr>
          <tr>
        <td>
          <br><subheading class="pub-year">2020</subheading>
        </td>
        </tr>
  <tr onmouseout="vpn_stop()" onmouseover="vpn_start()" bgcolor="#ffffd0">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/vpn.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function vpn_start() {
                  document.getElementById('vpn_image').style.opacity = "1";
                }
                function vpn_stop() {
                  document.getElementById('vpn_image').style.opacity = "0";
                }
                vpn_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2007.03056">
                <papertitle>VPN: Learning Video-Pose Embedding for Activities of Daily Living</papertitle>
              </a>
              <br>
              <strong>Srijan Das</strong>, Saurav Sharma, Rui Dai, Francois Bremond, Monique Thonnat.
              <br>
              <em><!--16th European Conference on Computer Vision</em>, 2020.-->
                <strong>ECCV 2020</strong>
              <br>
              <br>
              <a href="https://github.com/srijandas07/VPN">Code</a>
            </td>
          </tr>

          <tr onmouseout="wacv20_stop()" onmouseover="wacv20_start()">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/temporal_model.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function wacv20_start() {
                  document.getElementById('wacv20_image').style.opacity = "1";
                }
                function wacv20_stop() {
                  document.getElementById('wacv20_image').style.opacity = "0";
                }
                tsu_stop()
              </script>
           </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <a href="https://openaccess.thecvf.com/content_WACV_2020/html/Das_Looking_deeper_into_Time_for_Activities_of_Daily_Living_Recognition_WACV_2020_paper.html">
                     <papertitle>Looking deeper into Time for Activities of Daily Living Recognition</papertitle>
               </a>
              <br>
              <strong>Srijan Das</strong>, Monique Tonnat and Francois Bremond.
              <br>
              <em> <!--Winter Conference on Applications of Computer Vision, 2020. -->
                <strong>WACV 2020</strong></em>
              <br>
            </td>
          </tr>
        <tr>
        <td>
          <br><subheading class="pub-year">2019</subheading>
        </td>
        </tr>
        <tr onmouseout="sta_stop()" onmouseover="sta_start()" bgcolor="#ffffd0">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/STA_fig.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function stego_start() {
                  document.getElementById('sta_image').style.opacity = "1";
                }

                function stego_stop() {
                  document.getElementById('sta_image').style.opacity = "0";
                }
                stego_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Das_Toyota_Smarthome_Real-World_Activities_of_Daily_Living_ICCV_2019_paper.pdf">
                <papertitle>Toyota Smarthome: Real World Activities of Daily Living.</papertitle>
              </a>
              <br>
              <strong>Srijan Das</strong>, Rui Dai, Michal Koperski, Luca Minciullo, Lorenzo Garattoni, Francois Bremond and Gianpiero Francesca.
              <br>
              <em><!--In Proceedings of the 17th International Conference on Computer Vision</em>, 2019.-->
                <strong>ICCV 2019</strong>
              <br>
              <br>
              <a href="https://project.inria.fr/toyotasmarthome/">Project Link</a> / <a href="https://github.com/DAIGroup/separable_STA">Code</a>
            </td>
          </tr>
          <tr onmouseout="wacv19_stop()" onmouseover="wacv19_start()">
            <td style="padding:70px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/P-i3d.png' height=175px width=250px>
              </div>
              <script type="text/javascript">
                function wacv19_start() {
                  document.getElementById('wacv19_image').style.opacity = "1";
                }
                function wacv19_stop() {
                  document.getElementById('wacv19_image').style.opacity = "0";
                }
                tsu_stop()
              </script>
           </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <a href="https://ieeexplore.ieee.org/document/8658564">
                     <papertitle>Where to focus on for Human Action Recognition?</papertitle>
               </a>
              <br>
              <strong>Srijan Das</strong>, Arpit Chaudhary, Francois Bremond and Monique Thonnat.
              <br>
              <em> <!--Winter Conference on Applications of Computer Vision, 2019. -->
                <strong>WACV 2019</strong></em>
              <br>
            </td>
          </tr>
        </p>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Talks</heading>
              <p style="margin-left: 40px">
              <ul>
                <li><b><em>Jun 2023</em></b> &nbsp;&nbsp; Invited Talk on "Computer Vision for Robot Learning" as part of the "AI and Machine Vision for Robotics" short-term training program at IIITDM. (Virtually)  </li>
                <li><b><em>Apr 2023</em></b> &nbsp;&nbsp; Talk on "From Few to More: Enhancing ViT Performance on Limited Data" at PHPC Lab in UNC Charlotte.  </li>
                <li><b><em>Mar 2023</em></b> &nbsp;&nbsp; Talk on "From Pixels to Robots: Recipes for Vision-Enabled Robot Learning" at the Seminar on Controls and Robotics in UNC Charlotte.  </li>
                <li><b><em>Jan 2023</em></b> &nbsp;&nbsp; Talk on "Quo vadis, computer vision!" at the PhD seminar in UNC Charlotte.  </li>
                <li><b><em>Mar 2022</em></b> &nbsp;&nbsp; Invited Talk in AICTE sponsored Short Term Course on "Multiple Modalities are all you need for Video Understanding!" at IIITDM Kancheepuram. (Virtually)  </li>
                <li><b><em>Sep 2021</em></b> &nbsp;&nbsp; Talk on "Vision for understanding Activities of Daily Living" at <a href="https://www.linkedin.com/company/scitech-talks/" color="blue">SciTech Talks </a>. <a href="https://www.youtube.com/watch?v=YNFUMQkWBSk" color="blue">[video] </a> </li>
                <li><b><em>Apr 2021</em></b> &nbsp;&nbsp; Seminar talk on "How to combine modalities for understanding Activities of Daily Living? " for CSE 600 at Stony Brook University, NY, USA. </li>
                <li><b><em>Nov 2020</em></b> &nbsp;&nbsp; Seminar talk on "How to combine RGB & Poses for understanding Activities of Daily Living?" at Université Lumière Lyon 2. </li>
                <li><b><em>Nov 2019</em></b> &nbsp;&nbsp; <a href="https://www.meetup.com/fr-FR/Data-Science-Meetup-Nice-Sophia-Antipolis/events/266551193/" color="blue">Nice Data Science meetup </a>. <a href="https://drive.google.com/file/d/1mJe3Vh75wdZojeS3dxLPNOYWUIyi0c5O/view" color="blue">[slides] </a></li>
                <li><b><em>Aug 2018</em></b> &nbsp;&nbsp; <a href="http://www.innovation-alzheimer.fr/summer-school-program-2018/" color="blue">Summer School Brain Innovation Generation @ UCA </a>.  <a href="https://www.dropbox.com/s/ci4m4o2cqkpt85e/UCA_summer_school.pdf?dl=0" color="blue">[slides] </a></li>
              </ul>
              </p>
            </td>
          </tr>
        </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Teaching</heading>
                <p style="margin-left: 40px">
              <ul>
                <li><b><em>Fall 2022, Spring 2023, Fall 2023</em></b> <a href="data/4152_5152_Syllabus.pdf">ITCS 4152/5152 Computer Vision</a> </li>

                <li><b><em>Aug 2021</em></b> &nbsp;&nbsp; Surviving the Deep Learning Apocalypse (SKFGI Webinar series 2020) </li>
                <p style="margin-left: 80px">
              <ol type="1">
              <li>  Video Understanding: How to model Time? <a href="https://www.youtube.com/watch?v=bNINKn6S3Gs" color="blue"> [video]</a> </li>
              <li>  Tips to attend attention! <a href="https://www.youtube.com/watch?v=iBx9EvCdAC4" color="blue"> [video]</a> </li>
            </ol>
              <p>
                <li><b><em>Jan 2021</em></b> &nbsp;&nbsp; <a href="http://www-sop.inria.fr/members/Francois.Bremond/MSclass/deepLearningWinterSchool/UCA_master/index.html" color="blue">Deep Learning Winter School for Computer Vision 2019-20 </li> </p>
                <p style="margin-left: 80px">
              <ol type="1">
              <li>  Introduction to video classification & RNN. <a href="http://www-sop.inria.fr/members/Francois.Bremond/MSclass/deepLearningWinterSchool/slides/Lecture_Srijan.pdf" color="blue">[slides] </a> <a href="https://drive.google.com/file/d/1c4z9lAXdkqf1Ak7SHPtAWYwpg0hiVCQ8/view" color="blue">[assignment1] </a> <a href="https://colab.research.google.com/drive/1vZ8jaj5pMlHNnjF4KkBWf_7-lDpvxBeW" color="blue">[Practical 1]</a>  </li>
              <li>  Action Classification in videos <a href="http://www-sop.inria.fr/members/Francois.Bremond/MSclass/deepLearningWinterSchool/slides/Lecture_2.pdf" color="blue">[slides] </a> </li>
              <li>  Attention Mechanisms for video analytics <a href="http://www-sop.inria.fr/members/Francois.Bremond/MSclass/deepLearningWinterSchool/slides/Lecture_3.pdf" color="blue">[slides] </a> <a href="https://www.dropbox.com/s/miptig2m0qkbytg/Assignment_teaching.pdf?dl=0" color="blue">[assignment2] </a> <a href="http://www-sop.inria.fr/members/Francois.Bremond/MSclass/deepLearningWinterSchool/slides/Practical.pdf" color="blue">[Practical 2]</a> </li>
                </ol>
            </ul>
              </p>
            </td>
          </tr>
        </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Academic Activities</heading>
              <p style="margin-left: 40px">
              <ul>
                <li>Program committee member of AAAI-24 Student Program.</li>
                <li>Associate Editor for ICRA 2024.</li>
                <li>Member of DEI committe for CVPR 2023.</li>
                <li>Senior Program Committee Member for AAAI 2023 and AAAI 2024.</li>
                <li>Session chair for Image Understanding & Activity Recognition session at IPAS 2020. </li>
                <li>Mentored for B.E.N.J.I. in GirlScript Summer of Code 2019 edition. </li>
                <li>Mentor for the Emerging Technology Business Incubator (ETBI) Led by NIT Rourkela, a platform envisaged to transform the start-up ecosystem of the region. </li>
                <li>Reviewer at ICACIE 2017, 2018, SETIT 2018, KCST 2019, ICAML 2019, AVSS 2019, 2022, WACV 2020, 2021, 2022, CVPR 2021, 2022, 2023, 2024 ECCV 2022, ICCV 2021, 2023, AAAI 2023, NeurIPS 2023, IROS 2021.</li>
                <li>Reviewer at TPAMI, Patter Recognition, Elsevier Journal of CVIU, Elsevier Journal of FGCS, Elsevier Journal of Computer & Electrical Engineering, MTAP, and Journal of Signal Processing: Image Communication. </li>
                <li>Volunteer at ICACNI 2014, ICACNI 2016, ICCV 2019, ICLR 2020 & ICML 2020. </li>
                </ul>
              </p>
            </td>
          </tr>
        </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Thanks to Jon Barron for the theme.</a>
                <br>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>

</body>

</html>
